{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa52079",
   "metadata": {},
   "source": [
    "Pytorch 공부 사이트 : https://wikidocs.net/52460  \n",
    "\n",
    " 1. 목차  \n",
    "\t1) 단일변수 선형회귀(Simple Linear Regression) 복습  \n",
    "\t2) 다중변수 선형회귀(Multivariate Linear Regression) 이론  \n",
    "\t3) Navie Data Representation  \n",
    "\t4) Matrix Data Representation  \n",
    "    5) 다중변수 선형회귀(Multivariate Linear Regression)  \n",
    "    6) nn.Module : 모델 가정(Hypothesis) 간단히  \n",
    "    7) F.mse_loss : 손실 측정(Compute loss) 간단히  \n",
    " 2. 다중변수 선형 회귀(Multivariate Linear Regression) : 여러개의 데이터 다룸  \n",
    "\t1) 데이터 정의(Data definition)  \n",
    "<img src=\"./Data_definition.png\" width=\"200\" height=\"200\">  \n",
    "        (1) Traing dataset정의  \n",
    "            입력 : x_train = torch.FloatTensor([인덱스 범위])  \n",
    "            출력 : y_train = torch.FloatTensor([인덱스 범위])  \n",
    "        (2) Test dataset정의  \n",
    "            입력 : x_test =  \n",
    "            출력 : y_test =  \n",
    "    2) 모델 가정(Hypothesis)  \n",
    "<img src=\"./Multivariate_Linear_Regression.png\" width=\"400\" height=\"400\">  \n",
    "        (1) # 모델 가정(Hypothesis) 초기화  \n",
    "            W = torch.zeros((입력 차원, 출력 차원), requires_grad = True) ♣  \n",
    "            b = torch.zeros(입력 차원, requires_grad = True) ♣  \n",
    "            - 입력 차원, 출력차원  \n",
    "            - requires_grad=True : 학습할 것임을 명시  \n",
    "        (2) 모델 가정(Hypothesis) 예측  \n",
    "            // 1way 수작업 Hypothesis 계산 \n",
    "            hypothesis = x_train.matmul(W) + b  \n",
    "            - W : weight  \n",
    "            - b : Bias  \n",
    "            \n",
    "            // 2way nn.Module Hypothesis 계산 ♣  \n",
    "            import torch.nn as nn  \n",
    "            class MultivariateLinearRegressionModel(nn.Model):  \n",
    "                def __init__(self):  \n",
    "                    super().__init__()  \n",
    "                    self.linear == nn.Linear(3, 1)  \n",
    "                def forward(self, x):  \n",
    "                    return self.linear(x)  \n",
    "            hypothesis = model(x_train)  \n",
    "            - Hypothesis 계산 : forward()가 실행  \n",
    "            - Gradient 계산 : backward()가 실행  \n",
    "            \n",
    "    3) 손실 측정(Compute loss)  \n",
    "<img src=\"./Compute_loss.png\" width=\"400\" height=\"400\">  \n",
    "        (2) Compute loss(손실 측정) 계산  \n",
    "            // 1way 수작업  \n",
    "            Mean Squared Error(MSE) = cost(W, b) = torch.mean((hypothesis - y_train) ** 2)  \n",
    "            \n",
    "            // 2way F.mse_loss ♣  \n",
    "            import torch.nn.functional as F  \n",
    "            cost = F.mse_loss(prediction, y_train)  \n",
    "            - 쉽게 다른 loss와 교체 가능  \n",
    "            \n",
    "    4) 최적화(Optimize)  \n",
    "        (1) 최적화(Optimize) 설정 : optimizer = torch.optim.옵티마이저 종류([학습시킬 변수], lr=학습률) ♣  \n",
    "            - 확률적 경사하강법 : optimizer = torch.optim.SGD([W, b], lr=0.01) ♣  \n",
    "<img src=\"./Gradient_Descent.png\" width=\"400\" height=\"400\">  \n",
    "<img src=\"./Gradient_Descent2.png\" width=\"400\" height=\"400\">  \n",
    "        (2) Optimize(최적화)를 통한 학습  \n",
    "            - gradient 초기화 : optimizer.zero_grad() ♣  \n",
    "            - cost함수 미분 = cost가 감소하는 부분 =  gradient 계산 : cost.backward() ♣  \n",
    "            - W, b 업데이트 : optimizer.step() ♣  \n",
    "            \n",
    "    - (2)는 for epoch in range(nb_epochs)로 돌림  \n",
    "    5) 결과 출력  \n",
    "            print('Epoch {:4d}/{}, hypothesis: {}, cost: {:.6f}'.format(epoch, nb_epochs, \n",
    "                hypothesis.squeeze().detach(), cost.item())) ♣  \n",
    "    6) 딥러닝 목적  \n",
    "        - 최적 W, b 탐색  \n",
    "        - Cost 최소화  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7273524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "654394fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20, hypothesis: tensor([0., 0., 0., 0., 0.]), cost: 29661.800781\n",
      "Epoch    1/20, hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]), cost: 9298.520508\n",
      "Epoch    2/20, hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]), cost: 2915.712402\n",
      "Epoch    3/20, hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]), cost: 915.040527\n",
      "Epoch    4/20, hypothesis: tensor([137.7968, 165.6247, 163.1911, 177.7112, 126.3307]), cost: 287.936005\n",
      "Epoch    5/20, hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]), cost: 91.371010\n",
      "Epoch    6/20, hypothesis: tensor([148.1035, 178.0144, 175.3980, 191.0042, 135.7812]), cost: 29.758139\n",
      "Epoch    7/20, hypothesis: tensor([150.1744, 180.5042, 177.8508, 193.6753, 137.6805]), cost: 10.445305\n",
      "Epoch    8/20, hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]), cost: 4.391228\n",
      "Epoch    9/20, hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]), cost: 2.493135\n",
      "Epoch   10/20, hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]), cost: 1.897688\n",
      "Epoch   11/20, hypothesis: tensor([152.5485, 183.3610, 180.6640, 196.7389, 139.8602]), cost: 1.710541\n",
      "Epoch   12/20, hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]), cost: 1.651412\n",
      "Epoch   13/20, hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]), cost: 1.632387\n",
      "Epoch   14/20, hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]), cost: 1.625923\n",
      "Epoch   15/20, hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]), cost: 1.623412\n",
      "Epoch   16/20, hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]), cost: 1.622141\n",
      "Epoch   17/20, hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]), cost: 1.621253\n",
      "Epoch   18/20, hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0662, 140.0963]), cost: 1.620500\n",
      "Epoch   19/20, hypothesis: tensor([152.8014, 183.6715, 180.9666, 197.0686, 140.0985]), cost: 1.619770\n",
      "Epoch   20/20, hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.1000]), cost: 1.619033\n"
     ]
    }
   ],
   "source": [
    "# 전체 코드\n",
    "\n",
    "# 1) 데이터 정의\n",
    "x_train = torch.FloatTensor([[73, 80, 75], [93, 88, 93], [89, 91, 90], [96, 98, 100], [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# 2) 모델 초기화\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# 3) optimizer 설정\n",
    "optimizer = torch.optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    # 4) H(x) 계산\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "    \n",
    "    # 5) cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    # 6) cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch {:4d}/{}, hypothesis: {}, cost: {:.6f}'.format(epoch, nb_epochs, \n",
    "        hypothesis.squeeze().detach(), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e34cab65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    0/20, hypothesis: tensor([-43.1679, -51.2370, -50.8815, -54.3342, -39.8622]), cost: 48589.148438\n",
      "Epoch :    1/20, hypothesis: tensor([42.9147, 52.2286, 51.0646, 56.6825, 39.0562]), cost: 15231.794922\n",
      "Epoch :    2/20, hypothesis: tensor([ 91.1089, 110.1554, 108.1403, 118.8366,  83.2400]), cost: 4776.036133\n",
      "Epoch :    3/20, hypothesis: tensor([118.0909, 142.5866, 140.0949, 153.6343, 107.9770]), cost: 1498.710205\n",
      "Epoch :    4/20, hypothesis: tensor([133.1970, 160.7438, 157.9851, 173.1162, 121.8266]), cost: 471.442200\n",
      "Epoch :    5/20, hypothesis: tensor([141.6541, 170.9094, 168.0010, 184.0234, 129.5806]), cost: 149.447540\n",
      "Epoch :    6/20, hypothesis: tensor([146.3887, 176.6010, 173.6085, 190.1298, 133.9220]), cost: 48.518585\n",
      "Epoch :    7/20, hypothesis: tensor([149.0393, 179.7876, 176.7479, 193.5486, 136.3528]), cost: 16.882328\n",
      "Epoch :    8/20, hypothesis: tensor([150.5230, 181.5718, 178.5055, 195.4625, 137.7139]), cost: 6.965433\n",
      "Epoch :    9/20, hypothesis: tensor([151.3535, 182.5708, 179.4894, 196.5340, 138.4762]), cost: 3.856420\n",
      "Epoch :   10/20, hypothesis: tensor([151.8183, 183.1303, 180.0403, 197.1338, 138.9031]), cost: 2.881393\n",
      "Epoch :   11/20, hypothesis: tensor([152.0783, 183.4436, 180.3486, 197.4696, 139.1423]), cost: 2.575200\n",
      "Epoch :   12/20, hypothesis: tensor([152.2237, 183.6192, 180.5211, 197.6575, 139.2764]), cost: 2.478672\n",
      "Epoch :   13/20, hypothesis: tensor([152.3049, 183.7177, 180.6177, 197.7626, 139.3517]), cost: 2.447858\n",
      "Epoch :   14/20, hypothesis: tensor([152.3502, 183.7729, 180.6717, 197.8215, 139.3941]), cost: 2.437654\n",
      "Epoch :   15/20, hypothesis: tensor([152.3753, 183.8040, 180.7019, 197.8543, 139.4180]), cost: 2.433892\n",
      "Epoch :   16/20, hypothesis: tensor([152.3892, 183.8215, 180.7187, 197.8727, 139.4315]), cost: 2.432147\n",
      "Epoch :   17/20, hypothesis: tensor([152.3968, 183.8314, 180.7281, 197.8829, 139.4393]), cost: 2.431048\n",
      "Epoch :   18/20, hypothesis: tensor([152.4008, 183.8371, 180.7333, 197.8885, 139.4439]), cost: 2.430138\n",
      "Epoch :   19/20, hypothesis: tensor([152.4029, 183.8405, 180.7361, 197.8916, 139.4467]), cost: 2.429306\n",
      "Epoch :   20/20, hypothesis: tensor([152.4039, 183.8425, 180.7376, 197.8933, 139.4484]), cost: 2.428485\n"
     ]
    }
   ],
   "source": [
    "# 전체 코드\n",
    "\n",
    "# 1) 데이터 정의\n",
    "x_train = torch.FloatTensor([[73, 80, 75], [93, 88, 93], [89, 91, 90], [96, 98, 100], [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# 2) 모델 초기화\n",
    "# W = torch.zeros((3, 1), requires_grad=True)\n",
    "# b = torch.zeros(1, requires_grad=True)\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1) # 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = MultivariateLinearRegressionModel()\n",
    "\n",
    "# 3) Optimzer 설정\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    # 4) H(x) 계산\n",
    "    # hypothesis = x_train.matmul(W) + b\n",
    "    hypothesis = model(x_train)\n",
    "    \n",
    "    # 5) cost 계산\n",
    "    # cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    cost = F.mse_loss(hypothesis, y_train)\n",
    "    \n",
    "    # 6) cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 7) 출력\n",
    "    print('Epoch : {:4d}/{}, hypothesis: {}, cost: {:.6f}'.format(epoch, nb_epochs, \n",
    "                                                                hypothesis.squeeze().detach(), cost.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

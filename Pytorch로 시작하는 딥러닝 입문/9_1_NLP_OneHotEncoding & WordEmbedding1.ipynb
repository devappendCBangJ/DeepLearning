{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a837034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n● 자연어 처리 순서\\n    0. 훈련데이터, 검증 데이터, 테스트 데이터 분리\\n    1. 자연어 전처리\\n        1) 토큰화 : 주어진 문자열을 문자 단위로 자름\\n            (1) spacy\\n            (2) NLTK\\n            (3) 띄어쓰기\\n            (4) 한국어 띄어쓰기\\n            (5) 형태소 토큰화\\n            (6) 문자 토큰화\\n        2) 단어집합 생성\\n        3) 단어의 정수 인코딩\\n        4) 길이 다른 문장을 패딩\\n        5) 단어 벡터화\\n        6) 배치화 : 훈련 샘플들의 배치 생성 + 패딩\\n    2. 룩업 테이블(Lookup Table) : 단어들을 임베딩 벡터로 맵핑\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "● 자연어 처리 순서\n",
    "    0. 훈련데이터, 검증 데이터, 테스트 데이터 분리\n",
    "    1. 자연어 전처리\n",
    "        1) 토큰화 : 주어진 문자열을 문자 단위로 자름\n",
    "            (1) spacy\n",
    "            (2) NLTK\n",
    "            (3) 띄어쓰기\n",
    "            (4) 한국어 띄어쓰기\n",
    "            (5) 형태소 토큰화\n",
    "            (6) 문자 토큰화\n",
    "        2) 단어집합 생성\n",
    "        3) 단어의 정수 인코딩\n",
    "        4) 길이 다른 문장을 패딩\n",
    "        5) 단어 벡터화\n",
    "        6) 배치화 : 훈련 샘플들의 배치 생성 + 패딩\n",
    "    2. 룩업 테이블(Lookup Table) : 단어들을 임베딩 벡터로 맵핑\n",
    "\n",
    "● NLP 원핫 인코딩(One-Hot encoding)\n",
    "    1. 원핫 인코딩 특징\n",
    "        1) 개념 : 단어를 원핫 벡터로 표현 by 희소 표현\n",
    "            - 컴퓨터는 숫자가 문자보다 수월하게 처리한다. 그렇기 때문에 원핫 인코딩 필요\n",
    "            - 단어 집합 : 서로 다른 단어들의 집합(중복 허용x)\n",
    "        2) 특징\n",
    "            1] 희소표현(Sparse Representation)\n",
    "                - 값의 타입 : 1과 0\n",
    "                    표현하고 싶은 단어 인덱스 위치에 1, 다른 언어에 0으로 표현하는 벡터 표현\n",
    "                    >> 단어간 유사도 표현 불가(서로를 완전히 다른 단어로 인식)\n",
    "                        ex. 웹 검색 시스템에서 '삿포로 숙소' 검색시 비슷한 '삿포로 게스트 하우스'는 완전히 다른 것으로 인식\n",
    "                - 벡터 차원 : 고차원(단어 집합의 크기)\n",
    "                    벡터 차원 = 단어 집합의 크기\n",
    "                    >> 저장공간 낮은 효율성\n",
    "                        ex. 문장에 1000개의 단어가 있을 때, 단어 각각은 해당 인덱스에 1, 나머지는 0으로 표시\n",
    "                - 표현 방법 : 수동\n",
    "    2. 원핫 인코딩 순서\n",
    "        1) 각 단어 정수 인코딩(고유 인덱스 부여)\n",
    "        2) 각 단어 원핫 벡터 표현 by 희소 표현\n",
    "            \n",
    "● 워드 임베딩(Word Embedding)\n",
    "    1. 워드 임베딩 특징\n",
    "        1) 개념 : 단어를 임베딩 벡터로 표현 by 밀집 표현\n",
    "            - 컴퓨터는 숫자가 문자보다 수월하게 처리한다. 그렇기 때문에 워드 임베딩 필요\n",
    "            - 단어 집합 : 서로 다른 단어들의 집합(중복 허용x)\n",
    "        2) 특징\n",
    "            1] 밀집표현 = 분산표현(Dense Representation)\n",
    "                - 값의 타입 : 실수\n",
    "                    사용자가 설정한 값으로 모든 단어의 벡터 표현 차원 맞추므로 0, 1만이 아닌 실수값 벡터 표현\n",
    "                    >> 단어간 유사도 표현 가능\n",
    "                - 차원 : 저차원\n",
    "                    벡터 차원 != 단어 집합의 크기\n",
    "                    >> 저장공간 높은 효율성\n",
    "                - 표현 방법 : 훈련 데이터로부터 학습\n",
    "        3) 종류\n",
    "            - LSA\n",
    "                1) 개념 : 카운트 기반 방법. 전체 단어 빈도수 통계 기반으로 잠재 의미 파악\n",
    "                2) 특징\n",
    "                    - 입력 : 각 단어의 빈도수를 카운트한 행렬이라는 전체적 통계 정보\n",
    "                    - 과정 : 차원을 축소(Truncated SVD)하여 잠재된 의미 끌어냄\n",
    "                    - 장점 : 전체적 통계 정보 고려\n",
    "                    - 단점 : 유추 작업에 낮은 성능 (전체적 통계 정보 고려)\n",
    "            - Word2Vec\n",
    "                1) 개념 : 예측기반 방법. 손실함수 이용하여 실제값과 예측값에 대한 오차 줄여나감\n",
    "                2) 특징\n",
    "                    - 과정 : 실제값과 예측값에 대한 오차를 손실함수를 통해 줄여나가며 학습\n",
    "                    - 장점 : 유추 작업에 높은 성능 (윈도우 크기 내의 단어만 고려)\n",
    "                    - 단점 : 전체적 통계 정보 고려x (윈도우 크기 내의 단어만 고려)\n",
    "                # Word2Vec 체험 사이트 : http://w.elnn.kr/search/\n",
    "                # Word2Vec 중에서 임베딩 벡터 체험 사이트 : https://arxiv.org/pdf/1611.05469v1.pdf\n",
    "                3) 종류\n",
    "                    - CBOW\n",
    "                        1) 개념 : 주변 단어 입력. 중간 단어 예측\n",
    "                        2) 특징\n",
    "                            - 예시 : \"The fat cat sat on the mat\"에서 {\"The\", \"fat\", \"cat\", \"on\", \"the\", \"mat\"} 사이 sat 예측\n",
    "                            1] 얕은 신경망(Shallow Network)\n",
    "                                [1] 흐름 : input layer >> projection layer >> output layer\n",
    "                                    - 윈도우(Window) : 중간단어 기준으로 좌우 살펴볼 단어 범위 n\n",
    "                                    - 슬라이딩 윈도우(Window) : 중간 단어와 주변단어를 바꿔가며 학습을 위한 데이터셋 제작\n",
    "                                    - input layer의 입력\n",
    "                                        주변 단어 : context word. 원핫 벡터 표현\n",
    "                                    - output layer의 출력\n",
    "                                        중간 단어 : center word. 원핫 벡터 표현\n",
    "                                - 구체적인 이론은 https://wikidocs.net/60854 에서 확인\n",
    "                    - Skip-Gram\n",
    "                        1) 개념 : 중간 단어 입력. 주변 단어 예측\n",
    "                        2) 특징\n",
    "                            - 예시 : \"The fat cat sat on the mat\"에서 {\"sat\"} 주변 The, fat, cat, on 예측\n",
    "                            1] 얕은 신경망(Shallow Network)\n",
    "                                [1] 흐름 : input layer >> projection layer >> output layer\n",
    "                                    - 윈도우(Window) : 중간단어 기준으로 살펴볼 단어 범위 n\n",
    "                                    - 슬라이딩 윈도우(Window) : 중간 단어와 주변단어를 바꿔가며 학습을 위한 데이터셋 제작\n",
    "                                    - input layer의 입력\n",
    "                                        중간 단어 : center word. 원핫 벡터 표현\n",
    "                                    - output layer의 출력\n",
    "                                        주변 단어 : context word. 원핫 벡터 표현\n",
    "                                - 구체적인 이론은 https://wikidocs.net/60854 에서 확인\n",
    "                    - 네거티브 샘플링(Skip-Gram with Negative Sampling)\n",
    "                        1) 개념 : 중간 단어 입력. 주변 단어 예측\n",
    "                        2) 특징\n",
    "                            - 예시 : \"The fat cat sat on the mat\"에서 {\"sat\"} 주변 The, fat, cat, on 예측\n",
    "                            1] 얕은 신경망(Shallow Network)\n",
    "                                [1] 흐름 : input layer >> projection layer >> output layer\n",
    "                                    - 윈도우(Window) : 중간단어 기준으로 살펴볼 단어 범위 n\n",
    "                                    - 슬라이딩 윈도우(Window) : 중간 단어와 주변단어를 바꿔가며 학습을 위한 데이터셋 제작\n",
    "                                    - input layer의 입력\n",
    "                                        중간 단어 : center word. 원핫 벡터 표현\n",
    "                                    - output layer의 출력\n",
    "                                        주변 단어 : context word. 원핫 벡터 표현\n",
    "                                - 구체적인 이론은 https://wikidocs.net/60854 에서 확인\n",
    "            - FastText\n",
    "            - Glove\n",
    "                1) 개념 : LSA의 카운트 기반 방법 + Word2Vec의 예측기반 방법\n",
    "                    (임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것)\n",
    "                2) 특징\n",
    "                    [1] 윈도우 기반 동시 등장 행렬(Window based Co-occurrence Matrix)\n",
    "                        - 행, 열 : 전체 단어 집합의 단어들\n",
    "                        - i번째 행, k번째 열 : i단어의 윈도우 크기 내에서 k단어가 등장한 횟수\n",
    "                        - 이 행렬은 전치 해도 같다\n",
    "                    [2] 동시 등장 확률(Co-occurrence Probability)\n",
    "                        - 동시 등장 확률 P(k l i) : 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수 카운트, \n",
    "                            특정 단어 i가 등장했을 때, 어떤 단어 k가 등장한 횟수 카운트하여 계산한 조건부 확률\n",
    "                        >> i = 중심단어, k = 주변단어 라고 하면 동시 등장 행렬에서 \n",
    "                            분모 : 중심 단어 i 행의 모든 값을 더한 값\n",
    "                            분자 : i행 k열의 값\n",
    "                    [3] 손실함수\n",
    "                        - 구체적인 이론은 https://wikidocs.net/60858 에서 확인\n",
    "            - 임베딩 벡터\n",
    "                1) 개념\n",
    "                2) 특징\n",
    "                3) 사용 순서\n",
    "                    어떤 단어 >> 단어 정수 인코딩 >> 임베딩 층 통과 >> 밀집 벡터 = 임베딩 벡터\n",
    "                4) 종류\n",
    "                    - nn.embedding()\n",
    "                        1) 개념 : 처음부터 학습하는 임베딩 벡터(특화된 훈련 데이터 충분)\n",
    "                            - 임베딩 층(embedding layer) 만들어서 훈련 데이터로부터 처음부터 임베딩 벡터 학습\n",
    "                        2) 특징\n",
    "                            - 구체적인 이론은 https://wikidocs.net/64779 에서 확인\n",
    "                    - pre-trained word embedding\n",
    "                        1) 개념 : 사전 훈련되어있는 임베딩 벡터(특화된 훈련 데이터 부족)\n",
    "                        2) 특징\n",
    "                            \n",
    "                \n",
    "    2. 워드 임베딩 순서\n",
    "        1) 각 단어 정수 인코딩(고유 인덱스 부여)\n",
    "        2) 각 단어 임베딩 벡터 표현 by 밀집 표현\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7cb2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce4d62e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '자연어', '처리', '를', '배운다']\n",
      "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원핫 인코딩\n",
    "\n",
    "# 형태소 분석 + 문장 토큰화\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "token = okt.morphs(\"나는 자연어 처리를 배운다\")\n",
    "print(token)\n",
    "\n",
    "# 정수 인코딩 + 원핫 벡터 생성\n",
    "word2index = {}\n",
    "for voca in token: # 이거 좀 신박하네\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index)\n",
    "print(word2index)\n",
    "\n",
    "def one_hot_encoding(word, word2index):\n",
    "    one_hot_vector = [0]*(len(word2index))\n",
    "    index = word2index[word]\n",
    "    one_hot_vector[index] = 1\n",
    "    return one_hot_vector\n",
    "one_hot_encoding(\"자연어\", word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eff2be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# 원핫 인코딩 단점 예시\n",
    "import torch\n",
    "\n",
    "# 원핫 벡터 생성\n",
    "dog = torch.FloatTensor([1, 0, 0, 0, 0])\n",
    "cat = torch.FloatTensor([0, 1, 0, 0, 0])\n",
    "computer = torch.FloatTensor([0, 0, 1, 0, 0])\n",
    "netbook = torch.FloatTensor([0, 0, 0, 1, 0])\n",
    "book = torch.FloatTensor([0, 0, 0, 0, 1])\n",
    "\n",
    "# 원핫 벡터간 코사인 유사도\n",
    "print(torch.cosine_similarity(dog, cat, dim=0))\n",
    "print(torch.cosine_similarity(cat, computer, dim=0))\n",
    "print(torch.cosine_similarity(computer, netbook, dim=0))\n",
    "print(torch.cosine_similarity(netbook, book, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c33dd500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting glove_python_binary\n",
      "  Downloading glove_python_binary-0.2.0-cp38-cp38-win_amd64.whl (244 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\bang\\anaconda\\lib\\site-packages (from glove_python_binary) (1.6.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\bang\\anaconda\\lib\\site-packages (from glove_python_binary) (1.20.1)\n",
      "Installing collected packages: glove-python-binary\n",
      "Successfully installed glove-python-binary-0.2.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "임베딩 벡터 시각화를 위한 데이터 설치\n",
    "아나콘다 프롬프트 창 : conda install -c anaconda gensim\n",
    "\n",
    "!pip install gensim==3.4.0\n",
    "!pip install smart_open==1.9.0\n",
    "\n",
    "!python -m gensim.scripts.word2vec2tensor --input eng_w2v --output eng_w2v\n",
    "\n",
    "워드 임베딩 - GloVe\n",
    "!pip install glove_python_binary\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20f948f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 벡터 시각화 예시 : https://wikidocs.net/60856    ###eng_w2v가 없어서 진행 불가###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60499e78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d807c3c98167>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 윈도우 기반 동시 등장 행렬 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# GloVe 학습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "###result 변수가 없어서 진행 불가###\n",
    "# 워드 임베딩 - GloVe\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "# 윈도우 기반 동시 등장 행렬 생성\n",
    "corpus = Corpus()\n",
    "corpus.fit(result, window=5)\n",
    "\n",
    "# GloVe 학습\n",
    "glove = Glove(no_components=100, learning_rate=0.05) # 요소 개수 100, 학습률 0.05\n",
    "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True) # 윈도우 기반 동시 등장 행렬, 에포크 20, 스레드 4개\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "\n",
    "# 유사 단어 리스트 리턴\n",
    "model_result1=glove.most_similar(\"man\") # 입력단어와 유사한 단어 리스트 리턴\n",
    "print(model_result1)\n",
    "\n",
    "model_result2=glove.most_similar('boy')\n",
    "print(model_result2)\n",
    "\n",
    "model_result3=glove.most_similar('university')\n",
    "print(model_result3)\n",
    "\n",
    "model_result4=glove.most_similar('water')\n",
    "print(model_result4)\n",
    "\n",
    "model_result5=glove.most_similar('physics')\n",
    "print(model_result5)\n",
    "\n",
    "model_result6=glove.most_similar('muscle')\n",
    "print(model_result6)\n",
    "\n",
    "model_result7=glove.most_similar('clean')\n",
    "print(model_result7)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb7485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

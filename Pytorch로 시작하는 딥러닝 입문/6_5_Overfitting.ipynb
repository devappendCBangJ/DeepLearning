{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12dd43d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n● Overfitting : 훈련 데이터에 대한 지나친 학습(노이즈까지 학습)\\n\\n1. Overfitting 해결방안\\n    1) 데이터 양 증가\\n        - 적은 데이터 양 : 특정 패턴, 노이즈까지 학습\\n        - 적은 데이터 양 + 데이터 증식 or 데이터 증강(Data Augmentation) : 가상 노이즈 추가 or 일부 수정하여 일반적 패턴 학습\\n        - 많은 데이터 양 : 일반적 패턴 학습\\n    2) 인공신경망 복잡도 감소\\n        - 많은 인공신경망 : Overfitting\\n        - 적당한 인공신경망 : Fitting\\n        - 적은 인공신경망 : Underfitting\\n    3) 가중치 규제(Regulariztion)\\n        - 복잡한 모델(많은 매개변수) : Overfitting\\n        - 복잡한 모델(많은 매개변수) + 가중치 규제(Regularization)\\n            L1규제(L1노름) : 가중치 w들의 절대값 합계를 비용함수에 추가\\n                L1의 비용함수 : λㅣwl\\n                L1규제 장점 : 모델에 영향을 주는 매개변수 정확히 파악 but L2규제보다 성능 더 낮음\\n                    (가중치 w가 0이 되거나 0에 가까워짐)\\n            L2규제(L2노름) = 가중치 감쇠 : 모든 가중치 w들의 제곱합을 비용함수에 추가\\n                L2의 비용함수 : 1/2λw^2\\n                L2규제 장점 : 모델에 영향을 주는 특성들 정확히 파악불가 but L1규제보다 성능 더 높음\\n                    (가중치 w가 0은 되지 않지만 0에 가까워짐)\\n            - L1규제와 L2 규제의 공통점\\n                λ : 가중치 규제 강도 설정\\n                    λ 큼 : 모델에 영향을 주는 매개변수 파악보다 규제를 위해 추가된 항 작게 유지하는 것 우선\\n                    λ 작음 : 모델에 영향을 주는 매개변수 파악이 규제를 위해 추가된 항 작게 유지하는 것보다 우선\\n                비용함수 최소화 목표\\n                    비용함수 최소 되는 가중치, 편향 탐색\\n                    가중치들의 절대값의 합 or 제곱합이 최소가 되는 가중치, 편향 탐색\\n                    >> 가중치 w가 0에 가까워짐\\n                \\n        - 간단한 모델(적은 매개변수) : Fitting\\n    4) 드롭아웃(Dropout)\\n        - 드롭아웃 : 학습 과정에서 신경망 일부 빵꾸내서 사용x\\n            특정 뉴런 or 특정 조합에 의존 방지\\n        ex. 드롭아웃 비율 0.5 >> 학습 과정마다 랜덤으로 절반의 뉴런 사용x\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "■ DeepLearning TroubleShooting\n",
    "    ● Overfitting : 훈련 데이터에 대한 지나친 학습(노이즈까지 학습)\n",
    "        1. Overfitting 해결방안\n",
    "            1) 데이터 양 증가\n",
    "                - 적은 데이터 양 : 특정 패턴, 노이즈까지 학습\n",
    "                - 적은 데이터 양 + 데이터 증식 or 데이터 증강(Data Augmentation) : 가상 노이즈 추가 or 일부 수정하여 일반적 패턴 학습\n",
    "                - 많은 데이터 양 : 일반적 패턴 학습\n",
    "            2) 인공신경망 복잡도 감소\n",
    "                - 많은 인공신경망 : Overfitting\n",
    "                - 적당한 인공신경망 : Fitting\n",
    "                - 적은 인공신경망 : Underfitting\n",
    "            3) 가중치 규제(Regulariztion)\n",
    "                - 복잡한 모델(많은 매개변수) : Overfitting\n",
    "                - 복잡한 모델(많은 매개변수) + 가중치 규제(Regularization)\n",
    "                    L1규제(L1노름) : 가중치 w들의 절대값 합계를 비용함수에 추가\n",
    "                        L1의 비용함수 : λㅣwl\n",
    "                        L1규제 장점 : 모델에 영향을 주는 매개변수 정확히 파악 but L2규제보다 성능 더 낮음\n",
    "                            (가중치 w가 0이 되거나 0에 가까워짐)\n",
    "                    L2규제(L2노름) = 가중치 감쇠 : 모든 가중치 w들의 제곱합을 비용함수에 추가\n",
    "                        L2의 비용함수 : 1/2λw^2\n",
    "                        L2규제 장점 : 모델에 영향을 주는 특성들 정확히 파악불가 but L1규제보다 성능 더 높음\n",
    "                            (가중치 w가 0은 되지 않지만 0에 가까워짐)\n",
    "                    - L1규제와 L2 규제의 공통점\n",
    "                        λ : 가중치 규제 강도 설정\n",
    "                            λ 큼 : 모델에 영향을 주는 매개변수 파악보다 규제를 위해 추가된 항 작게 유지하는 것 우선\n",
    "                            λ 작음 : 모델에 영향을 주는 매개변수 파악이 규제를 위해 추가된 항 작게 유지하는 것보다 우선\n",
    "                        비용함수 최소화 목표\n",
    "                            비용함수 최소 되는 가중치, 편향 탐색\n",
    "                            가중치들의 절대값의 합 or 제곱합이 최소가 되는 가중치, 편향 탐색\n",
    "                            >> 가중치 w가 0에 가까워짐\n",
    "\n",
    "                - 간단한 모델(적은 매개변수) : Fitting\n",
    "            4) 드롭아웃(Dropout)\n",
    "                - 드롭아웃 : 학습 과정에서 신경망 일부 빵꾸냄\n",
    "                    특정 뉴런 or 특정 조합에 의존 방지\n",
    "                ex. 드롭아웃 비율 0.5 >> 학습 과정마다 랜덤으로 절반의 뉴런 사용x\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b8b971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 3개 선형 레이어 신경망\n",
    "class Architecture1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Architecture1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "# 만약에 여기서 과적합이 발생한다면???\n",
    "    \n",
    "# 2개 선형 레이어 신경망\n",
    "class Architecture1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Architecture1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e1bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

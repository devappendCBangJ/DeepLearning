{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cdaf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "■ DeepLearning TroubleShooting\n",
    "    ● 기울기 소실(Gradient Vanishing), 폭주(Exploding)\n",
    "        1. 기울기 소실, 폭주 개념\n",
    "            1) 기울기 소실 : 역전파 과정에서 입력층으로 갈수록 기울기(Gradient)가 너무 소실되어 가중치 소실\n",
    "            2) 폭주 : 역전파 과정에서 입력층으로 갈수록 기울기(Gradient)가 너무 증가되어 가중치 발산\n",
    "        2. 기울기 소실, 폭주 해결방안\n",
    "            1) ReLU or Leaky ReLU in 은닉층\n",
    "                1] 특징\n",
    "                    - Sigmoid 함수 or 하이퍼볼릭 탄젠트 함수 in 은닉층\n",
    "                        입력의 절대값이 크면 시그모이드 함수 출력값이 0 or 1 수렴\n",
    "                        >> 기울기가 0에 가까워짐\n",
    "                        >> 역전파 과정에서 전파시킬 기울기 소실\n",
    "                        >> 입력층 방향으로 갈수록 역전파 성능 감소\n",
    "                    - ReLU 함수 in 은닉층\n",
    "                        입력의 값이 0 이하면 ReLU 함수 출력값이 0 수렴\n",
    "                        >> 기울기가 0이 됨\n",
    "                        >> 역전파 과정에서 전파시킬 기울기 없어짐\n",
    "                        >> 입력층 방향으로 갈수록 역전파 성능 없어짐\n",
    "                        >> 죽은 ReLU\n",
    "                    - Leaky ReLU 함수 in 은닉층\n",
    "                        입력의 값과 상관없이 Leaky ReLU 출력값이 0 수렴x\n",
    "            2) 가중치 초기화(Weight initialization)\n",
    "                (1) 세이비어 초기화(Xavier initialization) = 글로럿 초기화(Glorot initialization)\n",
    "                    1] 특징\n",
    "                        - 여러 층 기울기 분산 사이의 균형 조절\n",
    "                        - S자 형태 함수(시그모이드 함수, 하이퍼볼릭 탄젠트 함수)에 높은 성능\n",
    "                        - ReLU 형태 함수(ReLU, Leaky ReLU)에 낮은 성능\n",
    "                    2] 종류\n",
    "                        - 균등분포 초기화\n",
    "                            W ~ Uniform(-{6/(n_in + n_out)}^(1/2), +{6/(n_in + n_out)}^(1/2))\n",
    "                            - n_in : 이전층의 뉴런 개수\n",
    "                            - n_out : 다음층의 뉴런 개수\n",
    "                        - 정규분포 초기화\n",
    "                            W ~ σ(표준편차) = {2/(n_in + n_out)}^(1/2)\n",
    "                (2) He 초기화(He initialization)\n",
    "                    1] 특징\n",
    "                        - 여러 층 기울기 분산 사이의 균형 조절\n",
    "                        - ReLU 형태 함수(ReLU, Leaky ReLU)에 높은 성능\n",
    "                        - S자 형태 함수(시그모이드 함수, 하이퍼볼릭 탄젠트 함수)에 낮은 성능\n",
    "                    2] 종류\n",
    "                        - 균등분포 초기화\n",
    "                            W ~ Uniform(-(6/n_in)^(1/2), +(-6/n_in)^(1/2))\n",
    "                        - 정규분포 초기화\n",
    "                            W ~ σ(표준편차) = (2/n_in)^(1/2)\n",
    "            3) 배치 정규화(Batch Normalization)\n",
    "                (1) 내부 공변량 변화(Internal Covariate Shift)\n",
    "                    1] 특징\n",
    "                        - 공변량 변화 : 훈련 데이터의 분포와 테스트 데이터의 분포가 다른 경우\n",
    "                        - 내부 공변량 변화 : 신경망 층 사이에서 발생하는 입력 데이터 분포 변화\n",
    "                (2) 배치 정규화(Batch Normalization)\n",
    "                    1] 특징\n",
    "                        - 활성화 함수 통과 전, 배치 단위 정규화\n",
    "                        - 시그모이드 함수, 하이퍼볼릭탄젠트 함수 기울기 소실문제 크게 개선\n",
    "                        - 가중치 초기화에 훨씬 덜 민감해짐\n",
    "                        - 훨씬 큰 학습률 사용 가능하여 학습 속도 개선\n",
    "                        - 미니배치마다 평균, 표준편차 계산으로 훈련데이터에 노이즈 넣는 과적합 방지 효과\n",
    "                            but 부수적 효과이기 때문에 드롭아웃과 함께 사용하는 것이 좋음\n",
    "                        \n",
    "                        입력\n",
    "                        >> 입력에 대한 평균 0으로 만듦, 정규화\n",
    "                        >> 정규화된 데이터에 대한 스케일, 시프트 수행\n",
    "                        이때, 두 개의 매개변수 γ(스케일을 위한 매개변수)와 β(시프트를 위한 매개변수) 사용\n",
    "                        >> 활성화 함수로 가자\n",
    "                    2] 한계\n",
    "                        - 미니배치 크기에 의존\n",
    "                            미니배치 : 동일한 특성 개수들을 가진 다수의 샘플들\n",
    "                            미니배치 크기 매우 작음 : 분산 0에 가까움 >> 배치 정규화 매우 극단적 작용\n",
    "                            미니배치 크기 적당 : 배치 정규화 효율적\n",
    "                        - RNN에 적용 어려움\n",
    "                            RNN : 각 시점(time step)마다 다른 통계치 지니기 때문에 배치 정규화 어려움\n",
    "                        - 모델 복잡화 >> 실행시간 증가\n",
    "                            \n",
    "            4) 층 정규화(Layer Normalization)\n",
    "                1] 특징\n",
    "                        - 활성화 함수 통과 전, 배치가 아닌 층 단위 정규화\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

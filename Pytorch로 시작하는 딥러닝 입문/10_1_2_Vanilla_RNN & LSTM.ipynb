{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a36682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n● 피드 포워드 신경망(Feed Forward Neural Network)\\n    1. 피드 포워드 신경망 특징\\n        1) 개념\\n        2) 특징\\n            - 방향 : 은닉층의 노드에서 활성화 함수를 지난값이 출력층 방향으로만 향함.\\n            - 단위\\n                입력층 : 뉴런\\n                출력층 : 뉴런\\n                은닉층 : 뉴런\\n\\n● 순환신경망(RNN = Recurrent Neural Network)\\n    1. 순환신경망 특징\\n        1) 개념 : 딥러닝의 기본적인 시퀀스 모델\\n            - d : 단어 벡터의 차원\\n            - D_h : 은닉 상태 크기\\n            - b : 편향\\n\\n            - x_t : 입력벡터\\n            - y_t : 출력벡터\\n            - h_t : 은닉 상태값 in 현재 시점 t\\n                h_t = tanh(W_x*x_t+W_h*h_(t-1) + b)\\n                    = (D_h x d matrix) x (d x 1 matrix) + (D_h x D_h matrix) x (D_h x 1 matrix) + (D_h x 1 matrix) = (D_h x 1 matrix)\\n                y_t = f(W_y*h_t + b)\\n                f = 비선형 활성화 함수\\n            - W_x : 입력층에서 입력값을 위한 가중치\\n            - W_h : h_(t-1)를 위한 가중치\\n\\n            - 위의 식에서 W_x, W_h, W_y는 모든 시점에서 같은 값 공유\\n            - 은닉층이 2개 이상인 경우 은닉층 2개의 가중치는 서로 다름\\n        2) 특징\\n            - 입력과 출력을 시퀀스 단위 처리\\n            - 활성화 함수\\n                h_t 계산 : 하이퍼볼릭탄젠트 함수\\n                y_t 계산\\n                    이진분류 : 시그모이드 함수\\n                    다중선택 : 소프트맥스 함수\\n            - 방향 : 은닉층의 노드에서 활성화 함수를 지난값이 출력층 방향으로 향함 + \\n                        은닉층 노드의 다음 계산의 입력값으로 보내짐\\n            - 단위\\n                입력층 : 입력벡터 x\\n                출력층 : 출력벡터 y\\n                은닉층 : 은닉상태(hidden state)\\n                    은닉상태 : 메모리 셀이 출력층 방향으로 다음 시점 t+1의 자신에게 보내는 값\\n                    (t 시점의 메모리 셀은 t-1 시점의 메모리 셀이 보낸 은닉값을 t시점의 은닉 상태 계산을 위한 입력값으로 사용)\\n            - RNN cell(= 메모리 셀) : 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드\\n            - 주의사항 : 재귀신경망(Recursive Neural Network)와는 다른 개념\\n        3) 종류 + 활용\\n            (1) 일대다(one-to-many) : 입력 한개, 출력 여러개 가능\\n                - 이미지 캡셔닝\\n                    입력 : 하나의 이미지\\n                    출력 : 사진 제목\\n            (2) 다대일(many-to-one) : 입력 여러개, 출력 하나 가능\\n                - 감성분류(sentiment classification)\\n                    입력 : 문서\\n                    출력 : 긍정 or 부정 판별\\n                - 스팸메일분류(spam detection)\\n                    입력 : 메일\\n                    출력 : 정상 or 스팸 판별\\n            (3) 다대다(many-to-many) : 입력 여러개, 출력 여러개 가능\\n                - 번역기\\n                    입력 : 번역할 문장\\n                    출력 : 번역된 문장\\n                - 챗봇\\n                    입력 : 명령 문장\\n                    출력 : 대답 문장\\n                - 개체명 인식, 품사 태깅 등\\n                    입력 : 문장 or 이미지\\n                    출력 : 품사 or 개체명\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "● 피드 포워드 신경망(Feed Forward Neural Network)\n",
    "    1. 피드 포워드 신경망 특징\n",
    "        1) 개념\n",
    "        2) 특징\n",
    "            - 방향 : 은닉층의 노드에서 활성화 함수를 지난값이 출력층 방향으로만 향함.\n",
    "            - 단위\n",
    "                입력층 : 뉴런\n",
    "                출력층 : 뉴런\n",
    "                은닉층 : 뉴런\n",
    "\n",
    "● 바닐라 순환신경망(Vanilla RNN = Vanilla Recurrent Neural Network)\n",
    "    1. 바닐라 순환신경망 특징\n",
    "        1) 개념 : 딥러닝의 기본적인 시퀀스 모델\n",
    "            - 함수\n",
    "                tanh : 하이퍼볼릭탄젠트 함수\n",
    "\n",
    "                D_h : 은닉 상태 크기\n",
    "                d : 단어 벡터의 차원\n",
    "\n",
    "            - 입출력 벡터, 은닉 상태값\n",
    "                x_t : 입력벡터 (d x 1 matrix)\n",
    "                y_t : 출력벡터\n",
    "                h_t : 은닉 상태값 in 현재 시점 t (D_h x 1 matrix)\n",
    "                    h_t = tanh(W_x*x_t+W_h*h_(t-1) + b)\n",
    "                        = (D_h x d matrix) x (d x 1 matrix) + (D_h x D_h matrix) x (D_h x 1 matrix) + (D_h x 1 matrix) = (D_h x 1 matrix)\n",
    "                    y_t = f(W_y*h_t + b)\n",
    "                    f = 비선형 활성화 함수\n",
    "\n",
    "            - 가중치와 편향\n",
    "                W_x : 입력층에서 입력벡터를 위한 가중치 (D_h x d matrix)\n",
    "                W_h : h_(t-1)를 위한 가중치 (D_h x D_h matrix)\n",
    "                b : 편향 (D_h x 1 matrix)\n",
    "                - 위의 식에서 W_x, W_h, W_y는 모든 시점에서 같은 값 공유\n",
    "                - 은닉층이 2개 이상인 경우 은닉층 2개의 가중치는 서로 다름\n",
    "        2) 특징\n",
    "            - 입력과 출력을 짧은 시퀀스 단위 처리\n",
    "            - 출력 결과가 이전의 계산 결과에 의존\n",
    "            1] 장점 : 짧은 시퀀스에서 높은 효과\n",
    "            1] 단점 : 긴 시퀀스에서 낮은 효과(장기 의존성 문제)\n",
    "            2] 입력값 방향 : 은닉층의 노드에서 활성화 함수를 지난값이 출력층 방향으로 향함 + \n",
    "                                은닉층 노드의 다음 계산의 입력값으로 보내짐\n",
    "            3] 단위\n",
    "                [1] 입력층 : 입력벡터 x\n",
    "                [2] 출력층 : 출력벡터 y\n",
    "                [3] 은닉층 : 은닉상태(hidden state)\n",
    "                [4] 은닉상태 : 메모리 셀이 출력층 방향으로 다음 시점 t+1의 자신에게 보내는 값\n",
    "                    (t 시점의 메모리 셀은 t-1 시점의 메모리 셀이 보낸 은닉값을 t시점의 은닉 상태 계산을 위한 입력값으로 사용)\n",
    "                [5] RNN cell(= 메모리 셀) : 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드\n",
    "            4] 활성화 함수\n",
    "                h_t 계산 : 하이퍼볼릭탄젠트 함수\n",
    "                y_t 계산\n",
    "                    이진분류 : 시그모이드 함수\n",
    "                    다중선택 : 소프트맥스 함수\n",
    "            - 주의사항 : 재귀신경망(Recursive Neural Network)와는 다른 개념\n",
    "        3) 종류 + 활용\n",
    "            (1) 활용 데이터 시점 + 은닉층 개수\n",
    "                1] 일반 RNN(Reccurent Neural Network) : 과거시점 데이터 고려 + 1개 은닉층\n",
    "                2] DRNN(Deep Neural Network) : 과거시점 데이터 고려 + 다수 은닉층\n",
    "                3] BRNN(Bidirectional Neural Network) : 모든시점 데이터 고려 + 1개 은닉층\n",
    "                4] DBRNN(Deep Bidirectinal Neural Network) : 모든시점 데이터 고려 + 다수 은닉층\n",
    "                    - 2개 메모리셀 사용\n",
    "                        과거 -> 미래 방향 메모리셀\n",
    "                        미래 -> 과거 방향 메모리셀\n",
    "\n",
    "            (2) 입출력 개수\n",
    "                1] 일대다(one-to-many) : 입력 한개, 출력 여러개 가능\n",
    "                    - 이미지 캡셔닝\n",
    "                        입력 : 하나의 이미지\n",
    "                        출력 : 사진 제목\n",
    "                2] 다대일(many-to-one) : 입력 여러개, 출력 하나 가능\n",
    "                    - 감성분류(sentiment classification)\n",
    "                        입력 : 문서\n",
    "                        출력 : 긍정 or 부정 판별\n",
    "                    - 스팸메일분류(spam detection)\n",
    "                        입력 : 메일\n",
    "                        출력 : 정상 or 스팸 판별\n",
    "                3] 다대다(many-to-many) : 입력 여러개, 출력 여러개 가능\n",
    "                    - 번역기\n",
    "                        입력 : 번역할 문장\n",
    "                        출력 : 번역된 문장\n",
    "                    - 챗봇\n",
    "                        입력 : 명령 문장\n",
    "                        출력 : 대답 문장\n",
    "                    - 개체명 인식, 품사 태깅 등\n",
    "                        입력 : 문장 or 이미지\n",
    "                        출력 : 품사 or 개체명\n",
    "\n",
    "● 장단기 메모리 = 변형 순환신경망(LSTM = Long Short-Term Memory)\n",
    "    1. 변형 순환신경망 특징\n",
    "        1) 개념 : 딥러닝의 기본적인 시퀀스 모델\n",
    "            - 함수\n",
    "                σ : 시그모이드 함수\n",
    "                tanh : 하이퍼볼릭탄젠트 함수\n",
    "\n",
    "                D_h : 은닉 상태 크기\n",
    "                d : 단어 벡터의 차원\n",
    "\n",
    "            - 입출력 벡터, 은닉 상태값\n",
    "                x_t : 입력벡터 (d x 1 matrix)\n",
    "                y_t : 출력벡터\n",
    "                h_t : 은닉 상태값 in 현재 시점 t (D_h x 1 matrix)\n",
    "                    h_t = tanh(W_x*x_t+W_h*h_(t-1) + b)\n",
    "                        = (D_h x d matrix) x (d x 1 matrix) + (D_h x D_h matrix) x (D_h x 1 matrix) + (D_h x 1 matrix) = (D_h x 1 matrix)\n",
    "                    y_t = f(W_y*h_t + b)\n",
    "                    f = 비선형 활성화 함수\n",
    "\n",
    "            - 가중치와 편향\n",
    "                W_xi, W_xg, W_xf, W_xo 4개 : 입력층에서 입력벡터를 위한 가중치 (D_h x d matrix)\n",
    "                W_hi, W_hg, W_hf, W_ho 4개 : h_(t-1)를 위한 가중치 (D_h x D_h matrix)\n",
    "                b_i, b_g, b_f, b_o : 편향 (D_h x 1 matrix)\n",
    "                - 위의 식에서 W_x, W_h, W_y는 모든 시점에서 같은 값 공유\n",
    "                - 은닉층이 2개 이상인 경우 은닉층 2개의 가중치는 서로 다름\n",
    "                \n",
    "            (1) 입력 게이트 : 현재 정보 기억(현재 입력 정보를 얼마나 반영할 것인가?)\n",
    "                i_t = σ(W_xi * x_t + W_hi * h_(t-1) + b_i)\n",
    "                    - 결과 : 0 or 1\n",
    "                g_t = tanh(W_xg * x_t + W_hg * h_(t-1) + b_g)\n",
    "                    - 결과 : -1 or 1\n",
    "            (2) 삭제 게이트 : 기억 삭제(이전 상태값 정보를 얼마나 반영할 것인가?)\n",
    "                f_t = σ(W_xf * x_t + W_hf * h_(t-1) + b_f)\n",
    "                    - 결과 : 0 or 1\n",
    "                        0에 가까운 값 : 정보 많이 삭제\n",
    "                        1에 가까운 값 : 정보 온전히 기억\n",
    "            (3) 셀 상태(장기 상태) : 삭제 게이트와 입력 게이트 영향력 조절(이전 시점 입력과 현재 시점 입력의 밸런스 조절)\n",
    "                C_t = f_t ∘ C_(t-1) + i_t ∘ g_t\n",
    "                    - ∘ : 원소별 곱 진행\n",
    "                    - 과정\n",
    "                        f_t가 0에 가까운 값(입력 게이트만 열음) : 이전 상태값 C_(t-1) 영향력 0에 가까워짐. 입력 게이트 결과의 영향력 1에 가까워짐\n",
    "                        i_t가 0에 가까운 값(삭제 게이트만 열음) : 이전 상태값 C_(t-1) 영향력 1에 가까워짐. 입력 게이트 결과의 영향력 0에 가까워짐\n",
    "            (4) 출력 게이트 : 현재 시점 t의 은닉 상태(단기 상태) 결정\n",
    "                o_t = σ(W_xo * x_t + W_ho * h_(t-1) + b_o)\n",
    "                    - 결과 : 0 or 1\n",
    "            (5) 은닉 상태(단기 상태)\n",
    "                h_t = o_t ∘ tanh(C_t)\n",
    "                    - 결과 : o_t ∘ [-1 ~ 1]\n",
    "        2) 특징\n",
    "            - 입력과 출력을 짧은 시퀀스 단위 처리\n",
    "            - 출력 결과가 이전의 계산 결과에 의존\n",
    "            1] 장점 : 짧은 시퀀스에서 높은 효과\n",
    "            1] 단점 : 긴 시퀀스에서 낮은 효과(장기 의존성 문제)\n",
    "            2] 입력값 방향 : 은닉층의 노드에서 활성화 함수를 지난값이 출력층 방향으로 향함 + \n",
    "                                은닉층 노드의 다음 계산의 입력값으로 보내짐\n",
    "            3] 단위\n",
    "                [1] 입력층 : 입력벡터 x\n",
    "                [2] 출력층 : 출력벡터 y\n",
    "                [3] 은닉층 : 은닉상태(hidden state)\n",
    "                [4] 은닉상태 : 메모리 셀이 출력층 방향으로 다음 시점 t+1의 자신에게 보내는 값\n",
    "                    (t 시점의 메모리 셀은 t-1 시점의 메모리 셀이 보낸 은닉값을 t시점의 은닉 상태 계산을 위한 입력값으로 사용)\n",
    "                [5] RNN cell(= 메모리 셀) : 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드\n",
    "            4] 활성화 함수\n",
    "                h_t 계산 : 하이퍼볼릭탄젠트 함수\n",
    "                y_t 계산\n",
    "                    이진분류 : 시그모이드 함수\n",
    "                    다중선택 : 소프트맥스 함수\n",
    "            - 주의사항 : 재귀신경망(Recursive Neural Network)와는 다른 개념\n",
    "        3) 종류 + 활용\n",
    "            (1) 활용 데이터 시점 + 은닉층 개수\n",
    "                1] 일반 RNN(Reccurent Neural Network) : 과거시점 데이터 고려 + 1개 은닉층\n",
    "                2] DRNN(Deep Neural Network) : 과거시점 데이터 고려 + 다수 은닉층\n",
    "                3] BRNN(Bidirectional Neural Network) : 모든시점 데이터 고려 + 1개 은닉층\n",
    "                4] DBRNN(Deep Bidirectinal Neural Network) : 모든시점 데이터 고려 + 다수 은닉층\n",
    "                    - 2개 메모리셀 사용\n",
    "                        과거 -> 미래 방향 메모리셀\n",
    "                        미래 -> 과거 방향 메모리셀\n",
    "\n",
    "            (2) 입출력 개수\n",
    "                1] 일대다(one-to-many) : 입력 한개, 출력 여러개 가능\n",
    "                    - 이미지 캡셔닝\n",
    "                        입력 : 하나의 이미지\n",
    "                        출력 : 사진 제목\n",
    "                2] 다대일(many-to-one) : 입력 여러개, 출력 하나 가능\n",
    "                    - 감성분류(sentiment classification)\n",
    "                        입력 : 문서\n",
    "                        출력 : 긍정 or 부정 판별\n",
    "                    - 스팸메일분류(spam detection)\n",
    "                        입력 : 메일\n",
    "                        출력 : 정상 or 스팸 판별\n",
    "                3] 다대다(many-to-many) : 입력 여러개, 출력 여러개 가능\n",
    "                    - 번역기\n",
    "                        입력 : 번역할 문장\n",
    "                        출력 : 번역된 문장\n",
    "                    - 챗봇\n",
    "                        입력 : 명령 문장\n",
    "                        출력 : 대답 문장\n",
    "                    - 개체명 인식, 품사 태깅 등\n",
    "                        입력 : 문장 or 이미지\n",
    "                        출력 : 품사 or 개체명\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2cf9084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# RNN(Recurrent Neural Network) 의사코드\\n\\n# 은닉상태 초기화\\nhidden_state = 0\\n\\n# 각 시점마다 데이터 입력\\nfor input_t in input_length:\\n\\n    # 각 시점에서 은닉상태 연산 by t 시점 입력, 이전 은닉상태 이용\\n    output_t = tanh(input_t, hidden_state_t)\\n    hidden_state_t = output_t\\n\\n# input_length : 입력 데이터 길이 = 총 시점의 수(timesteps)\\n# input_t : t시점 입력 데이터\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# RNN(Recurrent Neural Network) 의사코드\n",
    "\n",
    "# 은닉상태 초기화\n",
    "hidden_state = 0\n",
    "\n",
    "# 메모리셀 구현\n",
    "# 각 시점마다 데이터 입력\n",
    "for input_t in input_length:\n",
    "\n",
    "    # 각 시점에서 은닉상태 연산 by t시점 입력 + 이전 은닉상태 이용\n",
    "    output_t = tanh(input_t, hidden_state_t)\n",
    "    hidden_state_t = output_t\n",
    "\n",
    "# input_length : 입력 데이터 길이 = 총 시점 개수(timesteps)\n",
    "# input_t : t시점 입력 데이터\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9126e009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(8, 4)\n",
      "(8, 8)\n",
      "(8,)\n",
      "(1, 8)\n",
      "(2, 8)\n",
      "(3, 8)\n",
      "(4, 8)\n",
      "(5, 8)\n",
      "(6, 8)\n",
      "(7, 8)\n",
      "(8, 8)\n",
      "(9, 8)\n",
      "(10, 8)\n",
      "[[0.70973525 0.92129443 0.77883891 0.96104871 0.87075323 0.85210356\n",
      "  0.91486307 0.89541277]\n",
      " [0.99983806 0.99995467 0.99992939 0.99999102 0.99999258 0.99976072\n",
      "  0.99988121 0.99992247]\n",
      " [0.99995234 0.999967   0.99998201 0.99999733 0.99999741 0.99991031\n",
      "  0.99995667 0.9999775 ]\n",
      " [0.99998589 0.99999292 0.99999304 0.99999933 0.99999941 0.99997456\n",
      "  0.99997301 0.99998775]\n",
      " [0.99997606 0.99997382 0.99998976 0.99999837 0.99999902 0.99994516\n",
      "  0.99994367 0.99998093]\n",
      " [0.99997281 0.99998072 0.9999939  0.99999878 0.999999   0.99996018\n",
      "  0.99996205 0.99997987]\n",
      " [0.99998397 0.99998994 0.99999533 0.99999919 0.99999949 0.99997316\n",
      "  0.9999621  0.99998397]\n",
      " [0.99994788 0.99997838 0.99996864 0.99999506 0.99999719 0.99985788\n",
      "  0.99993617 0.99997613]\n",
      " [0.99991201 0.99997165 0.99997564 0.99999008 0.99999655 0.99979002\n",
      "  0.99989828 0.99996072]\n",
      " [0.99997516 0.99997665 0.99999305 0.99999852 0.99999913 0.99995346\n",
      "  0.99994665 0.99997917]]\n"
     ]
    }
   ],
   "source": [
    "# RNN(Recurrent Neural Network) Python 구현\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 하이퍼파라미터 초기화\n",
    "timesteps = 10 # 총 시점 개수. NLP에서는 문장의 길이\n",
    "input_size = 4 # 입력의 차원. NLP에서는 단어벡터 차원\n",
    "hidden_size = 8 # 은닉상태 크기. 메모리 셀 용량\n",
    "\n",
    "# 입력 2D 텐서\n",
    "inputs = np.random.random((timesteps, input_size)) # 10 x 4 matrix 2D 텐서\n",
    "    # 실제로는 Pytorch에서 (batch_size, timesteps, input_size) 3D 텐서 입력\n",
    "\n",
    "# 은닉상태 초기화\n",
    "hidden_state_t = np.zeros((hidden_size,))\n",
    "print(hidden_state_t)\n",
    "\n",
    "# 입력 가중치, 은닉상태 가중치, 편향 초기화\n",
    "Wx = np.random.random((hidden_size, input_size)) # 8 x 4 matrix 2D 텐서\n",
    "Wh = np.random.random((hidden_size, hidden_size)) # 8 x 8 matrix 2D 텐서\n",
    "b = np.random.random((hidden_size,)) # 8 x 1 matrix 1D 텐서\n",
    "print(np.shape(Wx))\n",
    "print(np.shape(Wh))\n",
    "print(np.shape(b))\n",
    "\n",
    "total_hidden_states = []\n",
    "\n",
    "# 메모리셀 구현\n",
    "# 각 시점마다 데이터 입력\n",
    "for input_t in inputs:\n",
    "    # 각 시점에서 은닉상태 연산 by t 시점 입력 + 이전 은닉상태 이용\n",
    "    output_t = np.tanh(np.dot(Wx, input_t) + np.dot(Wh, hidden_state_t) + b) # Wx * Xt + Wh * Ht-1 + b(bias)\n",
    "    hidden_state_t = output_t\n",
    "    # 각 시점의 은닉상태 축적 저장\n",
    "    total_hidden_states.append(list(output_t))\n",
    "    print(np.shape(total_hidden_states))\n",
    "# 깔끔한 출력\n",
    "total_hidden_states = np.stack(total_hidden_states, axis = 0)\n",
    "print(total_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a4680bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 8])\n",
      "torch.Size([1, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# RNN(Recurrent Neural Network) Pytorch 구현\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 하이퍼파라미터 초기화\n",
    "batch_size = 1 # 배치 사이즈\n",
    "time_steps = 10 # 총 시점 개수\n",
    "input_size = 5 # 입력의 차원\n",
    "hidden_size = 8 # 은닉상태 크기\n",
    "\n",
    "# 입력 3D 텐서\n",
    "inputs = torch.Tensor(batch_size, time_steps, input_size) # 1 x 10 x 5 matrix 3D 텐서\n",
    "\n",
    "# 메모리셀 구현\n",
    "cell = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "outputs, _status = cell(inputs)\n",
    "    # RNN셀의 리턴\n",
    "        # 1) 모든 시점(timesteps)의 은닉 상태들 : (배치 크기, 총 시점 개수, 은닉상태 크기)\n",
    "        # 2) 마지막 시점(timestep)의 은닉 상태 : (층 개수, 단일 시점 개수, 은닉상태 크기)\n",
    "print(outputs.shape) # 결과가 (1, 10, 8)이므로 10번의 시점동안 8차원의 은닉상태가 출력되었다는 뜻\n",
    "print(_status.shape) # 1번의 시점동안 8차원의 은닉상태가 출력되었다는 뜻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14e8fd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 8])\n",
      "torch.Size([2, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# 깊은 순환 신경망(Deep Recurrent Neural Network) Pytorch 구현\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 초기화\n",
    "batch_size = 1 # 배치 사이즈\n",
    "time_steps = 10 # 총 시점 개수\n",
    "input_size = 5 # 입력의 차원\n",
    "hidden_size = 8 # 은닉상태 크기\n",
    "num_layers = 2 # 은닉층 개수\n",
    "\n",
    "# 입력 3D 텐서\n",
    "inputs = torch.Tensor(batch_size, time_steps, input_size) # 1 x 10 x 5 matrix 3D 텐서\n",
    "\n",
    "# 메모리셀 구현\n",
    "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True)\n",
    "outputs, _status = cell(inputs)\n",
    "    # DRNN셀의 리턴\n",
    "        # 1) 모든 시점(timesteps)의 은닉 상태들 : (배치 크기, 총 시점 개수, 은닉상태 크기)\n",
    "        # 2) 마지막 시점(timestep)의 은닉 상태 : (층 개수, 단일 시점 개수, 은닉상태 크기)\n",
    "print(outputs.shape) # 결과가 (1, 10, 8)이므로 10번의 시점동안 8차원의 은닉상태가 출력되었다는 뜻\n",
    "print(_status.shape) # 1번의 시점동안 8차원의 은닉상태가 출력되었다는 뜻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70c31986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 16])\n",
      "torch.Size([4, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# 양방향 순환 신경망(Bidirectional Recurrent Neural Network) Pytorch 구현\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 하이퍼파라미터 초기화\n",
    "batch_size = 1\n",
    "time_steps = 10 # 총 시점 개수\n",
    "input_size = 5 # 입력의 차원\n",
    "hidden_size = 8 # 은닉상태 크기\n",
    "num_layers = 2\n",
    "\n",
    "# 입력 3D 텐서\n",
    "inputs = torch.Tensor(batch_size, time_steps, input_size) # 1 x 10 x 5 matrix 3D 텐서\n",
    "\n",
    "# 메모리셀 구현\n",
    "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True, bidirectional = True)\n",
    "outputs, _status = cell(inputs)\n",
    "    # BRNN셀의 리턴\n",
    "        # 1) 모든 시점(timesteps)의 은닉 상태들 : (배치 크기, 총 시점 개수, 은닉상태 크기*2)\n",
    "            # - 양방향의 은닉 상태값 연결\n",
    "        # 2) 마지막 시점(timestep)의 은닉 상태 : (층 개수*2, 단일 시점 개수, 은닉상태 크기)\n",
    "            # - 정방향 기준으로는 마지막 시점, 역방향 기준으로는 첫번째 시점에 해당되는 시점의 출력값을 층의 개수만큼 쌓음\n",
    "print(outputs.shape) # 결과가 (1, 10, 8)이므로 10번의 시점동안 8*2차원의 은닉상태가 출력되었다는 뜻\n",
    "print(_status.shape) # 1번의 시점동안 8차원의 은닉상태가 출력되었다는 뜻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb952355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 장단기 메모리 = 변형 순환 신경망(LSTM) Pytorch 구현\n",
    "    # RNN : nn.RNN(input_dim, hidden_size, batch_first=True)\n",
    "    # LSTM : nn.LSTM(input_dim, hidden_size, batch_first=True)\n",
    "    # - 이렇게 바꾸면 끝!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91e97815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n● 자연어 처리 순서\\n    0. 훈련데이터, 검증 데이터, 테스트 데이터 분리\\n    1. 자연어 전처리\\n        1) 토큰화 : 주어진 문자열을 문자 단위로 자름\\n            (1) spacy\\n            (2) NLTK\\n            (3) 띄어쓰기\\n            (4) 한국어 띄어쓰기\\n            (5) 형태소 토큰화\\n            (6) 문자 토큰화\\n        2) 단어집합 생성\\n        3) 단어의 정수 인코딩\\n        4) 길이 다른 문장을 패딩\\n        5) 단어 벡터화\\n        6) 배치화 : 훈련 샘플들의 배치 생성 + 패딩\\n    2. 룩업 테이블(Lookup Table) : 단어들을 임베딩 벡터로 맵핑\\n\\n● NLP 원핫 인코딩(One-Hot encoding)\\n    1. 원핫 인코딩 특징\\n        1) 개념 : 단어를 원핫 벡터로 표현 by 희소 표현\\n            - 컴퓨터는 숫자가 문자보다 수월하게 처리한다. 그렇기 때문에 원핫 인코딩 필요\\n            - 단어 집합 : 서로 다른 단어들의 집합(중복 허용x)\\n        2) 특징\\n            1] 희소표현(Sparse Representation)\\n                - 값의 타입 : 1과 0\\n                    표현하고 싶은 단어 인덱스 위치에 1, 다른 언어에 0으로 표현하는 벡터 표현\\n                    >> 단어간 유사도 표현 불가(서로를 완전히 다른 단어로 인식)\\n                        ex. 웹 검색 시스템에서 \\'삿포로 숙소\\' 검색시 비슷한 \\'삿포로 게스트 하우스\\'는 완전히 다른 것으로 인식\\n                - 벡터 차원 : 고차원(단어 집합의 크기)\\n                    벡터 차원 = 단어 집합의 크기\\n                    >> 저장공간 낮은 효율성\\n                        ex. 문장에 1000개의 단어가 있을 때, 단어 각각은 해당 인덱스에 1, 나머지는 0으로 표시\\n                - 표현 방법 : 수동\\n    2. 원핫 인코딩 순서\\n        1) 각 단어 정수 인코딩(고유 인덱스 부여)\\n        2) 각 단어 원핫 벡터 표현 by 희소 표현\\n            \\n● 워드 임베딩(Word Embedding)\\n    1. 워드 임베딩 특징\\n        1) 개념 : 단어를 임베딩 벡터로 표현 by 밀집 표현\\n            - 컴퓨터는 숫자가 문자보다 수월하게 처리한다. 그렇기 때문에 워드 임베딩 필요\\n            - 단어 집합 : 서로 다른 단어들의 집합(중복 허용x)\\n        2) 특징\\n            1] 밀집표현 = 분산표현(Dense Representation)\\n                - 값의 타입 : 실수\\n                    사용자가 설정한 값으로 모든 단어의 벡터 표현 차원 맞추므로 0, 1만이 아닌 실수값 벡터 표현\\n                    >> 단어간 유사도 표현 가능\\n                - 차원 : 저차원\\n                    벡터 차원 != 단어 집합의 크기\\n                    >> 저장공간 높은 효율성\\n                - 표현 방법 : 훈련 데이터로부터 학습\\n        3) 종류\\n            - LSA\\n                1) 개념 : 카운트 기반 방법. 전체 단어 빈도수 통계 기반으로 잠재 의미 파악\\n                2) 특징\\n                    - 입력 : 각 단어의 빈도수를 카운트한 행렬이라는 전체적 통계 정보\\n                    - 과정 : 차원을 축소(Truncated SVD)하여 잠재된 의미 끌어냄\\n                    - 장점 : 전체적 통계 정보 고려\\n                    - 단점 : 유추 작업에 낮은 성능 (전체적 통계 정보 고려)\\n            - Word2Vec\\n                1) 개념 : 예측기반 방법. 손실함수 이용하여 실제값과 예측값에 대한 오차 줄여나감\\n                2) 특징\\n                    - 과정 : 실제값과 예측값에 대한 오차를 손실함수를 통해 줄여나가며 학습\\n                    - 장점 : 유추 작업에 높은 성능 (윈도우 크기 내의 단어만 고려)\\n                    - 단점 : 전체적 통계 정보 고려x (윈도우 크기 내의 단어만 고려)\\n                # Word2Vec 체험 사이트 : http://w.elnn.kr/search/\\n                # Word2Vec 중에서 임베딩 벡터 체험 사이트 : https://arxiv.org/pdf/1611.05469v1.pdf\\n                3) 종류\\n                    - CBOW\\n                        1) 개념 : 주변 단어 입력. 중간 단어 예측\\n                        2) 특징\\n                            - 예시 : \"The fat cat sat on the mat\"에서 {\"The\", \"fat\", \"cat\", \"on\", \"the\", \"mat\"} 사이 sat 예측\\n                            1] 얕은 신경망(Shallow Network)\\n                                [1] 흐름 : input layer >> projection layer >> output layer\\n                                    - 윈도우(Window) : 중간단어 기준으로 좌우 살펴볼 단어 범위 n\\n                                    - 슬라이딩 윈도우(Window) : 중간 단어와 주변단어를 바꿔가며 학습을 위한 데이터셋 제작\\n                                    - input layer의 입력\\n                                        주변 단어 : context word. 원핫 벡터 표현\\n                                    - output layer의 출력\\n                                        중간 단어 : center word. 원핫 벡터 표현\\n                                - 구체적인 이론은 https://wikidocs.net/60854 에서 확인\\n                    - Skip-Gram\\n                        1) 개념 : 중간 단어 입력. 주변 단어 예측\\n                        2) 특징\\n                            - 예시 : \"The fat cat sat on the mat\"에서 {\"sat\"} 주변 The, fat, cat, on 예측\\n                            1] 얕은 신경망(Shallow Network)\\n                                [1] 흐름 : input layer >> projection layer >> output layer\\n                                    - 윈도우(Window) : 중간단어 기준으로 살펴볼 단어 범위 n\\n                                    - 슬라이딩 윈도우(Window) : 중간 단어와 주변단어를 바꿔가며 학습을 위한 데이터셋 제작\\n                                    - input layer의 입력\\n                                        중간 단어 : center word. 원핫 벡터 표현\\n                                    - output layer의 출력\\n                                        주변 단어 : context word. 원핫 벡터 표현\\n                                - 구체적인 이론은 https://wikidocs.net/60854 에서 확인\\n                    - 네거티브 샘플링(Skip-Gram with Negative Sampling)\\n                        1) 개념 : 중간 단어 입력. 주변 단어 예측\\n                        2) 특징\\n                            - 예시 : \"The fat cat sat on the mat\"에서 {\"sat\"} 주변 The, fat, cat, on 예측\\n                            1] 얕은 신경망(Shallow Network)\\n                                [1] 흐름 : input layer >> projection layer >> output layer\\n                                    - 윈도우(Window) : 중간단어 기준으로 살펴볼 단어 범위 n\\n                                    - 슬라이딩 윈도우(Window) : 중간 단어와 주변단어를 바꿔가며 학습을 위한 데이터셋 제작\\n                                    - input layer의 입력\\n                                        중간 단어 : center word. 원핫 벡터 표현\\n                                    - output layer의 출력\\n                                        주변 단어 : context word. 원핫 벡터 표현\\n                                - 구체적인 이론은 https://wikidocs.net/60854 에서 확인\\n            - FastText\\n            - Glove\\n                1) 개념 : LSA의 카운트 기반 방법 + Word2Vec의 예측기반 방법\\n                    (임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것)\\n                2) 특징\\n                    [1] 윈도우 기반 동시 등장 행렬(Window based Co-occurrence Matrix)\\n                        - 행, 열 : 전체 단어 집합의 단어들\\n                        - i번째 행, k번째 열 : i단어의 윈도우 크기 내에서 k단어가 등장한 횟수\\n                        - 이 행렬은 전치 해도 같다\\n                    [2] 동시 등장 확률(Co-occurrence Probability)\\n                        - 동시 등장 확률 P(k l i) : 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수 카운트, \\n                            특정 단어 i가 등장했을 때, 어떤 단어 k가 등장한 횟수 카운트하여 계산한 조건부 확률\\n                        >> i = 중심단어, k = 주변단어 라고 하면 동시 등장 행렬에서 \\n                            분모 : 중심 단어 i 행의 모든 값을 더한 값\\n                            분자 : i행 k열의 값\\n                    [3] 손실함수\\n                        - 구체적인 이론은 https://wikidocs.net/60858 에서 확인\\n            - 임베딩 벡터\\n                1) 개념\\n                2) 특징\\n                3) 사용 순서\\n                    어떤 단어 >> 단어 정수 인코딩 >> 임베딩 층 통과 >> 밀집 벡터 = 임베딩 벡터\\n                4) 종류\\n                    - nn.embedding()\\n                        1) 개념 : 처음부터 학습하는 임베딩 벡터(특화된 훈련 데이터 충분)\\n                            - 임베딩 층(embedding layer) 만들어서 훈련 데이터로부터 처음부터 임베딩 벡터 학습\\n                        2) 특징\\n                            - 구체적인 이론은 https://wikidocs.net/64779 에서 확인\\n                    - pre-trained word embedding\\n                        1) 개념 : 사전 훈련되어있는 임베딩 벡터(특화된 훈련 데이터 부족)\\n                        2) 특징\\n                \\n    2. 워드 임베딩 순서\\n        1) 각 단어 정수 인코딩(고유 인덱스 부여)\\n        2) 각 단어 임베딩 벡터 표현 by 밀집 표현\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "● 자연어 처리 순서\n",
    "    0. 훈련데이터, 검증 데이터, 테스트 데이터 분리\n",
    "    1. 자연어 전처리\n",
    "        1) 토큰화 : 주어진 문자열을 문자 단위로 자름\n",
    "            (1) spacy\n",
    "            (2) NLTK\n",
    "            (3) 띄어쓰기\n",
    "            (4) 한국어 띄어쓰기\n",
    "            (5) 형태소 토큰화\n",
    "            (6) 문자 토큰화\n",
    "        2) 단어집합 생성\n",
    "        3) 단어의 정수 인코딩\n",
    "        4) 길이 다른 문장을 패딩\n",
    "        5) 단어 벡터화\n",
    "        6) 배치화 : 훈련 샘플들의 배치 생성 + 패딩\n",
    "    2. 룩업 테이블(Lookup Table) : 단어들을 임베딩 벡터로 맵핑\n",
    "\n",
    "● NLP 원핫 인코딩(One-Hot encoding)\n",
    "    1. 원핫 인코딩 특징\n",
    "        1) 개념 : 단어를 원핫 벡터로 표현 by 희소 표현\n",
    "            - 컴퓨터는 숫자가 문자보다 수월하게 처리한다. 그렇기 때문에 원핫 인코딩 필요\n",
    "            - 단어 집합 : 서로 다른 단어들의 집합(중복 허용x)\n",
    "        2) 특징\n",
    "            1] 희소표현(Sparse Representation)\n",
    "                - 값의 타입 : 1과 0\n",
    "                    표현하고 싶은 단어 인덱스 위치에 1, 다른 언어에 0으로 표현하는 벡터 표현\n",
    "                    >> 단어간 유사도 표현 불가(서로를 완전히 다른 단어로 인식)\n",
    "                        ex. 웹 검색 시스템에서 '삿포로 숙소' 검색시 비슷한 '삿포로 게스트 하우스'는 완전히 다른 것으로 인식\n",
    "                - 벡터 차원 : 고차원(단어 집합의 크기)\n",
    "                    벡터 차원 = 단어 집합의 크기\n",
    "                    >> 저장공간 낮은 효율성\n",
    "                        ex. 문장에 1000개의 단어가 있을 때, 단어 각각은 해당 인덱스에 1, 나머지는 0으로 표시\n",
    "                - 표현 방법 : 수동\n",
    "    2. 원핫 인코딩 순서\n",
    "        1) 각 단어 정수 인코딩(고유 인덱스 부여)\n",
    "        2) 각 단어 원핫 벡터 표현 by 희소 표현\n",
    "            \n",
    "● 워드 임베딩(Word Embedding)\n",
    "    1. 워드 임베딩 특징\n",
    "        1) 개념 : 단어를 임베딩 벡터로 표현 by 밀집 표현\n",
    "            - 컴퓨터는 숫자가 문자보다 수월하게 처리한다. 그렇기 때문에 워드 임베딩 필요\n",
    "            - 단어 집합 : 서로 다른 단어들의 집합(중복 허용x)\n",
    "        2) 특징\n",
    "            1] 밀집표현 = 분산표현(Dense Representation)\n",
    "                - 값의 타입 : 실수\n",
    "                    사용자가 설정한 값으로 모든 단어의 벡터 표현 차원 맞추므로 0, 1만이 아닌 실수값 벡터 표현\n",
    "                    >> 단어간 유사도 표현 가능\n",
    "                - 차원 : 저차원\n",
    "                    벡터 차원 != 단어 집합의 크기\n",
    "                    >> 저장공간 높은 효율성\n",
    "                - 표현 방법 : 훈련 데이터로부터 학습\n",
    "        3) 종류\n",
    "            - LSA\n",
    "                1) 개념 : 카운트 기반 방법. 전체 단어 빈도수 통계 기반으로 잠재 의미 파악\n",
    "                2) 특징\n",
    "                    - 입력 : 각 단어의 빈도수를 카운트한 행렬이라는 전체적 통계 정보\n",
    "                    - 과정 : 차원을 축소(Truncated SVD)하여 잠재된 의미 끌어냄\n",
    "                    - 장점 : 전체적 통계 정보 고려\n",
    "                    - 단점 : 유추 작업에 낮은 성능 (전체적 통계 정보 고려)\n",
    "            - Word2Vec\n",
    "                1) 개념 : 예측기반 방법. 손실함수 이용하여 실제값과 예측값에 대한 오차 줄여나감\n",
    "                2) 특징\n",
    "                    - 과정 : 실제값과 예측값에 대한 오차를 손실함수를 통해 줄여나가며 학습\n",
    "                    - 장점 : 유추 작업에 높은 성능 (윈도우 크기 내의 단어만 고려)\n",
    "                    - 단점 : 전체적 통계 정보 고려x (윈도우 크기 내의 단어만 고려)\n",
    "                # Word2Vec 체험 사이트 : http://w.elnn.kr/search/\n",
    "                # Word2Vec 중에서 임베딩 벡터 체험 사이트 : https://arxiv.org/pdf/1611.05469v1.pdf\n",
    "                3) 종류\n",
    "                    - CBOW\n",
    "                        1) 개념 : 주변 단어 입력. 중간 단어 예측\n",
    "                        2) 특징\n",
    "                            - 예시 : \"The fat cat sat on the mat\"에서 {\"The\", \"fat\", \"cat\", \"on\", \"the\", \"mat\"} 사이 sat 예측\n",
    "                            1] 얕은 신경망(Shallow Network)\n",
    "                                [1] 흐름 : input layer >> projection layer >> output layer\n",
    "                                    - 윈도우(Window) : 중간단어 기준으로 좌우 살펴볼 단어 범위 n\n",
    "                                    - 슬라이딩 윈도우(Window) : 중간 단어와 주변단어를 바꿔가며 학습을 위한 데이터셋 제작\n",
    "                                    - input layer의 입력\n",
    "                                        주변 단어 : context word. 원핫 벡터 표현\n",
    "                                    - output layer의 출력\n",
    "                                        중간 단어 : center word. 원핫 벡터 표현\n",
    "                                - 구체적인 이론은 https://wikidocs.net/60854 에서 확인\n",
    "                    - Skip-Gram\n",
    "                        1) 개념 : 중간 단어 입력. 주변 단어 예측\n",
    "                        2) 특징\n",
    "                            - 예시 : \"The fat cat sat on the mat\"에서 {\"sat\"} 주변 The, fat, cat, on 예측\n",
    "                            1] 얕은 신경망(Shallow Network)\n",
    "                                [1] 흐름 : input layer >> projection layer >> output layer\n",
    "                                    - 윈도우(Window) : 중간단어 기준으로 살펴볼 단어 범위 n\n",
    "                                    - 슬라이딩 윈도우(Window) : 중간 단어와 주변단어를 바꿔가며 학습을 위한 데이터셋 제작\n",
    "                                    - input layer의 입력\n",
    "                                        중간 단어 : center word. 원핫 벡터 표현\n",
    "                                    - output layer의 출력\n",
    "                                        주변 단어 : context word. 원핫 벡터 표현\n",
    "                                - 구체적인 이론은 https://wikidocs.net/60854 에서 확인\n",
    "                    - 네거티브 샘플링(Skip-Gram with Negative Sampling)\n",
    "                        1) 개념 : 중간 단어 입력. 주변 단어 예측\n",
    "                        2) 특징\n",
    "                            - 예시 : \"The fat cat sat on the mat\"에서 {\"sat\"} 주변 The, fat, cat, on 예측\n",
    "                            1] 얕은 신경망(Shallow Network)\n",
    "                                [1] 흐름 : input layer >> projection layer >> output layer\n",
    "                                    - 윈도우(Window) : 중간단어 기준으로 살펴볼 단어 범위 n\n",
    "                                    - 슬라이딩 윈도우(Window) : 중간 단어와 주변단어를 바꿔가며 학습을 위한 데이터셋 제작\n",
    "                                    - input layer의 입력\n",
    "                                        중간 단어 : center word. 원핫 벡터 표현\n",
    "                                    - output layer의 출력\n",
    "                                        주변 단어 : context word. 원핫 벡터 표현\n",
    "                                - 구체적인 이론은 https://wikidocs.net/60854 에서 확인\n",
    "            - FastText\n",
    "            - Glove\n",
    "                1) 개념 : LSA의 카운트 기반 방법 + Word2Vec의 예측기반 방법\n",
    "                    (임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것)\n",
    "                2) 특징\n",
    "                    [1] 윈도우 기반 동시 등장 행렬(Window based Co-occurrence Matrix)\n",
    "                        - 행, 열 : 전체 단어 집합의 단어들\n",
    "                        - i번째 행, k번째 열 : i단어의 윈도우 크기 내에서 k단어가 등장한 횟수\n",
    "                        - 이 행렬은 전치 해도 같다\n",
    "                    [2] 동시 등장 확률(Co-occurrence Probability)\n",
    "                        - 동시 등장 확률 P(k l i) : 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수 카운트, \n",
    "                            특정 단어 i가 등장했을 때, 어떤 단어 k가 등장한 횟수 카운트하여 계산한 조건부 확률\n",
    "                        >> i = 중심단어, k = 주변단어 라고 하면 동시 등장 행렬에서 \n",
    "                            분모 : 중심 단어 i 행의 모든 값을 더한 값\n",
    "                            분자 : i행 k열의 값\n",
    "                    [3] 손실함수\n",
    "                        - 구체적인 이론은 https://wikidocs.net/60858 에서 확인\n",
    "            - 임베딩 벡터\n",
    "                1) 개념\n",
    "                2) 특징\n",
    "                3) 사용 순서\n",
    "                    어떤 단어 >> 단어 정수 인코딩 >> 임베딩 층 통과 >> 밀집 벡터 = 임베딩 벡터\n",
    "                4) 종류\n",
    "                    - nn.embedding()\n",
    "                        1) 개념 : 처음부터 학습하는 임베딩 벡터(특화된 훈련 데이터 충분)\n",
    "                            - 임베딩 층(embedding layer) 만들어서 훈련 데이터로부터 처음부터 임베딩 벡터 학습\n",
    "                        2) 특징\n",
    "                            - 구체적인 이론은 https://wikidocs.net/64779 에서 확인\n",
    "                    - pre-trained word embedding\n",
    "                        1) 개념 : 사전 훈련되어있는 임베딩 벡터(특화된 훈련 데이터 부족)\n",
    "                        2) 특징\n",
    "                            \n",
    "                \n",
    "    2. 워드 임베딩 순서\n",
    "        1) 각 단어 정수 인코딩(고유 인덱스 부여)\n",
    "        2) 각 단어 임베딩 벡터 표현 by 밀집 표현\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6103227f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'how': 2, 'you': 3, 'need': 4, 'code': 5, 'to': 6, 'know': 7, '<unk>': 0, '<pad>': 1}\n",
      "tensor([[0.1000, 0.5000, 0.7000],\n",
      "        [0.2000, 0.1000, 0.8000],\n",
      "        [0.1000, 0.8000, 0.9000],\n",
      "        [0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 룩업 테이블 과정 - nn.Embedding 사용x\n",
    "import torch\n",
    "\n",
    "# 학습 문자열\n",
    "train_data = 'you need to know how to code'\n",
    "\n",
    "# 단어집합 학습(중복 제거)\n",
    "word_set = set(train_data.split())\n",
    "\n",
    "# 정수 인코딩 학습\n",
    "vocab = {word: i+2 for i, word in enumerate(word_set)}\n",
    "vocab['<unk>'] = 0\n",
    "vocab['<pad>'] = 1\n",
    "print(vocab)\n",
    "\n",
    "# 임베딩 테이블(단어집합의 크기를 행으로 가짐) 생성\n",
    "embedding_table = torch.FloatTensor([\n",
    "                               [ 0.0,  0.0,  0.0],\n",
    "                               [ 0.0,  0.0,  0.0],\n",
    "                               [ 0.2,  0.9,  0.3],\n",
    "                               [ 0.1,  0.5,  0.7],\n",
    "                               [ 0.2,  0.1,  0.8],\n",
    "                               [ 0.4,  0.1,  0.1],\n",
    "                               [ 0.1,  0.8,  0.9],\n",
    "                               [ 0.6,  0.1,  0.1]])\n",
    "\n",
    "# 테스트 문자열\n",
    "sample = 'you need to run'.split()\n",
    "idxes = []\n",
    "\n",
    "# 정수 인코딩\n",
    "for word in sample:\n",
    "    try:\n",
    "        idxes.append(vocab[word])\n",
    "    except KeyError: # 단어집합에 없는 단어인 경우 <unk>로 대체\n",
    "        idxes.append(vocab['<unk>'])\n",
    "idxes = torch.LongTensor(idxes)\n",
    "\n",
    "# 정수 인코딩에 해당되는 값 임베딩 테이블에서 가져옴\n",
    "lookup_result = embedding_table[idxes, :]\n",
    "print(lookup_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "285e9ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'how': 2, 'you': 3, 'need': 4, 'code': 5, 'to': 6, 'know': 7, '<unk>': 0, '<pad>': 1}\n",
      "Parameter containing:\n",
      "tensor([[ 1.4599,  0.2334, -0.5782],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.3933, -1.1054,  0.1042],\n",
      "        [-0.2739,  1.0844,  0.0961],\n",
      "        [-0.4376,  0.8713,  0.3628],\n",
      "        [-1.6500, -0.5062,  0.8964],\n",
      "        [ 0.8575,  0.8507,  0.8194],\n",
      "        [ 1.6294,  0.6121,  1.3914]], requires_grad=True)\n",
      "tensor([[0.1000, 0.5000, 0.7000],\n",
      "        [0.2000, 0.1000, 0.8000],\n",
      "        [0.1000, 0.8000, 0.9000],\n",
      "        [0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 룩업 테이블 과정 - nn.Embedding 사용o\n",
    "import torch\n",
    "\n",
    "# 학습 문자열\n",
    "train_data = 'you need to know how to code'\n",
    "\n",
    "# 단어집합 학습(중복 제거)\n",
    "word_set = set(train_data.split())\n",
    "\n",
    "# 정수 인코딩 학습\n",
    "vocab = {word: i+2 for i, word in enumerate(word_set)}\n",
    "vocab['<unk>'] = 0\n",
    "vocab['<pad>'] = 1\n",
    "print(vocab)\n",
    "\n",
    "# 임베딩 테이블 생성\n",
    "import torch.nn as nn\n",
    "embedding_layer = nn.Embedding(num_embeddings=len(vocab),\n",
    "                                  embedding_dim = 3,\n",
    "                                  padding_idx = 1)\n",
    "    # num_embeddings : 단어집합 크기 = 임베딩할 단어들의 개수\n",
    "    # embedding_dim : 임베딩 할 벡터의 차원(사용자 지정 하이퍼파라미터)\n",
    "    # padding_idx : 패딩을 위한 토큰의 인덱스 알려줌(선택적으로 사용 가능)\n",
    "print(embedding_layer.weight)\n",
    "\n",
    "# 테스트 문자열\n",
    "sample = 'you need to run'.split()\n",
    "idxes = []\n",
    "\n",
    "# 정수 인코딩\n",
    "for word in sample:\n",
    "    try:\n",
    "        idxes.append(vocab[word])\n",
    "    except KeyError: # 단어집합에 없는 단어인 경우 <unk>로 대체\n",
    "        idxes.append(vocab['<unk>'])\n",
    "idxes = torch.LongTensor(idxes)\n",
    "\n",
    "# 정수 인코딩에 해당되는 값 임베딩 테이블에서 가져옴\n",
    "lookup_result = embedding_table[idxes, :]\n",
    "print(lookup_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fa4f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchmetrics.aggregation import MeanMetric\n",
    "from torchmetrics.functional.classification import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build config\n",
    "title = 'resnet101_finetune'\n",
    "device = 'cuda'\n",
    "root = 'data'\n",
    "batch_size = 128\n",
    "num_workers = 8\n",
    "lr = 0.01 # adam optimizer : 0.001이 SGD : 0.1과 비슷한 수준\n",
    "weight_decay = 1e-8\n",
    "label_smoothing = 0.05\n",
    "epochs = 20\n",
    "log_dir = 'logs'\n",
    "checkpoint_dir = 'checkpoints'\n",
    "pretrained_model = 'resnet101-cd907fc2.pth' # pytorch 공식 사이트에서 다운받을 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "train_transform = T.Compose([\n",
    "    T.RandomCrop(size=(32, 32), padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.TrivialAugmentWide(), # 다양한 augmentation을 확률적으로 적용\n",
    "    T.ToTensor(),\n",
    "    T.RandomErasing(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "train_data = CIFAR100(root, train=True, download=True, transform=train_transform)\n",
    "train_loader = DataLoader(train_data, batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "val_data = CIFAR100(root, train=False, download=True, transform=val_transform)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet building block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_dim, dim, stride=1, expansion=4):\n",
    "        super().__init__()\n",
    "        out_dim = dim * expansion\n",
    "        self.conv1 = nn.Conv2d(in_dim, dim, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(dim)\n",
    "        self.conv2 = nn.Conv2d(dim, dim, 3, stride, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(dim)\n",
    "        self.conv3 = nn.Conv2d(dim, out_dim, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        if stride != 1 or in_dim != out_dim: # 입력 행렬의 크기와 out 행렬의 크기를 맞춰주기 위함\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(dim, out_dim, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_dim),\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.downsample(x)\n",
    "        out = self.relu(self.bn1(self.conv1(x))) # conv -> batch_normalization -> relu\n",
    "        out = self.relu(self.bn2(self.conv2(out))) # conv -> batch_normalization -> relu\n",
    "        out = self.bn3(self.conv3(out)) # conv -> batch_normalization\n",
    "        out = out + identity # out + identity -> relu. 입력 행렬의 크기와 out 행렬의 크기가 같지 않으면 덧셈 연산 불가능\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module): # ResNet Network 101 구조를 구현\n",
    "    def __init__(self, depths):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3, bias=False)\n",
    "        # stride : 기존 resnet 이미지 크기(224 x 224)일때는 stride 2를 해서 이미지 크기를 줄였는데, cifar-10 이미지 크기(32 x 32)는 stride 2를하면 이미지 텐서가 남아나질 않아서 1로 수정\n",
    "        # padding : kernel size가 7이므로 padding을 3으로 해야 이미지 손실이 없음\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(64, 64, depths[0], stride=1) # depths[0] = 3\n",
    "        self.layer2 = self._make_layer(64 * 4, 128, depths[1], stride=2) # depths[1] = 4\n",
    "        self.layer3 = self._make_layer(128 * 4, 256, depths[2], stride=1) # depths[2] = 23\n",
    "        self.layer4 = self._make_layer(256 * 4, 512, depths[3], stride=2) # depths[3] = 3\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def _make_layer(self, in_dim, dim, depth, stride=1): # ♣ 그래서 모델 구조가 어떻게 되는거지???\n",
    "        layers = [Block(in_dim, dim, stride)]\n",
    "        layers.extend([Block(dim * 4, dim, 1) for _ in range(1, depth)])\n",
    "        # stride : stride는 위에서 한번만 적용되고, 나머지는 1로 두어야 이미지가 계속 작아지는 경우가 발생하지 않는다.\n",
    "        # in_dim : _make_layer함수 실행 전의 데이터 차원\n",
    "        # dim * 4 : out의 차원\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.relu(self.bn1(self.conv1(x)))) # conv -> batch_normalization -> relu -> maxpool\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = x.mean([-1, -2]) # Pytorch의 텐서 순서 : Batch x Channel x Height x Weight. 그러므로 -1 : Weight, -2 : Height\n",
    "        return x\n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet101(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = ResNet(depths=[3, 4, 23, 3])\n",
    "        self.head = nn.Linear(2048, 100) # conv 마지막 차원 -> 분류할 label 개수\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "model = ResNet101()\n",
    "state_dict = torch.load(pretrained_model)\n",
    "model.features.load_state_dict(state_dict, strict=False)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build optimizer \n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay, nesterov=True)\n",
    "\n",
    "# Build scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs * len(train_loader))\n",
    "\n",
    "# Build loss function\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "# Build metric function\n",
    "metric_fn = accuracy\n",
    "\n",
    "# Build logger\n",
    "train_logger = SummaryWriter(f'{log_dir}/train/{title}')\n",
    "val_logger = SummaryWriter(f'{log_dir}/val/{title}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop \n",
    "def train(loader, model, optimizer, scheduler, loss_fn, metric_fn, device):\n",
    "    model.train()\n",
    "    loss_mean = MeanMetric()\n",
    "    metric_mean = MeanMetric()\n",
    "    \n",
    "    for inputs, targets in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        metric = metric_fn(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_mean.update(loss.to('cpu'))\n",
    "        metric_mean.update(metric.to('cpu'))\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    summary = {'loss': loss_mean.compute(), 'metric': metric_mean.compute()}\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation loop \n",
    "def evaluate(loader, model, loss_fn, metric_fn, device):\n",
    "    model.eval()\n",
    "    loss_mean = MeanMetric()\n",
    "    metric_mean = MeanMetric()\n",
    "    \n",
    "    for inputs, targets in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        metric = metric_fn(outputs, targets)\n",
    "\n",
    "        loss_mean.update(loss.to('cpu'))\n",
    "        metric_mean.update(metric.to('cpu'))\n",
    "    \n",
    "    summary = {'loss': loss_mean.compute(), 'metric': metric_mean.compute()}\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop\n",
    "for epoch in range(epochs):\n",
    "    # train one epoch\n",
    "    train_summary = train(train_loader, model, optimizer, scheduler, loss_fn, metric_fn, device)\n",
    "    \n",
    "    # evaluate one epoch\n",
    "    val_summary = evaluate(val_loader, model, loss_fn, metric_fn, device)\n",
    "\n",
    "    # write log\n",
    "    train_logger.add_scalar('Loss', train_summary['loss'], epoch + 1)\n",
    "    train_logger.add_scalar('Accuracy', train_summary['metric'], epoch + 1)\n",
    "    val_logger.add_scalar('Loss', val_summary['loss'], epoch + 1)\n",
    "    val_logger.add_scalar('Accuracy', val_summary['metric'], epoch + 1)\n",
    "    \n",
    "    # save model\n",
    "    state_dict = {\n",
    "        'epoch': epoch + 1,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    checkpoint_path = f'{checkpoint_dir}/{title}_last.pth'\n",
    "    torch.save(state_dict, checkpoint_path)\n",
    "        \n",
    "train_logger.close()\n",
    "val_logger.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "47cd534c079f69b1fca6fc1b0efc6d00f8e8bb4d8dd2aa87a73e0a70b773969b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

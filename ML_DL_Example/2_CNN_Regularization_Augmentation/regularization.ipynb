{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchmetrics.aggregation import MeanMetric\n",
    "from torchmetrics.functional.classification import accuracy\n",
    "\n",
    "from src.models import ConvNet\n",
    "from src.engines import train, evaluate\n",
    "from src.utils import load_checkpoint, save_checkpoint\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--title\", type=str, default=\"regularization\")\n",
    "parser.add_argument(\"--device\", type=str, default=\"cuda\")\n",
    "parser.add_argument(\"--root\", type=str, default=\"data\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "parser.add_argument(\"--num_workers\", type=int, default=2)\n",
    "parser.add_argument(\"--epochs\", type=int, default=100)\n",
    "parser.add_argument(\"--lr\", type=float, default=0.001)\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=0.0001)\n",
    "parser.add_argument(\"--label_smoothing\", type=float, default=0.05)\n",
    "parser.add_argument(\"--drop_rate\", type=float, default=0.1)\n",
    "parser.add_argument(\"--logs\", type=str, default='logs')\n",
    "parser.add_argument(\"--checkpoints\", type=str, default='checkpoints')\n",
    "parser.add_argument(\"--resume\", type=bool, default=False)\n",
    "args = parser.parse_args()\n",
    "\n",
    "def main(args):\n",
    "    # Build dataset\n",
    "    train_transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    train_data = CIFAR10(args.root, train=True, download=True, transform=train_transform)\n",
    "    train_loader = DataLoader(train_data, args.batch_size, shuffle=True, num_workers=args.num_workers, drop_last=True)\n",
    "\n",
    "    val_transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    val_data = CIFAR10(args.root, train=False, download=True, transform=val_transform)\n",
    "    val_loader = DataLoader(val_data, batch_size=args.batch_size, num_workers=args.num_workers)\n",
    "\n",
    "    # Build model\n",
    "    model = ConvNet(drop_rate=args.drop_rate)\n",
    "    model = model.to(args.device)\n",
    "\n",
    "    # Build optimizer \n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    # Build scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs * len(train_loader))\n",
    "\n",
    "    # Build loss function\n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=args.label_smoothing)\n",
    "\n",
    "    # Build metric function\n",
    "    metric_fn = accuracy\n",
    "\n",
    "    # Build logger\n",
    "    train_logger = SummaryWriter(f'{args.logs}/train/{args.title}')\n",
    "    val_logger = SummaryWriter(f'{args.logs}/val/{args.title}')\n",
    "\n",
    "    # Load model\n",
    "    start_epoch = 0\n",
    "    if args.resume:\n",
    "        start_epoch = load_checkpoint(args.checkpoints, args.title, model, optimizer)\n",
    "    \n",
    "    # Main loop\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        # train one epoch\n",
    "        train_summary = train(train_loader, model, optimizer, scheduler, loss_fn, metric_fn, args.device)\n",
    "        \n",
    "        # evaluate one epoch\n",
    "        val_summary = evaluate(val_loader, model, loss_fn, metric_fn, args.device)\n",
    "\n",
    "        # write log\n",
    "        train_logger.add_scalar('Loss', train_summary['loss'], epoch + 1)\n",
    "        train_logger.add_scalar('Accuracy', train_summary['metric'], epoch + 1)\n",
    "        val_logger.add_scalar('Loss', val_summary['loss'], epoch + 1)\n",
    "        val_logger.add_scalar('Accuracy', val_summary['metric'], epoch + 1)\n",
    "\n",
    "        # save model\n",
    "        save_checkpoint(args.checkpoints, args.title, model, optimizer, epoch + 1)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "BangEnv",
   "language": "python",
   "name": "bangenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.transforms' has no attribute 'TrivialAugmentWide'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 120>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    117\u001b[0m         save_checkpoint(args\u001b[38;5;241m.\u001b[39mcheckpoints, args\u001b[38;5;241m.\u001b[39mtitle, model, optimizer, epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 121\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(args):\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# Build dataset\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     train_transform \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     59\u001b[0m         T\u001b[38;5;241m.\u001b[39mRandomCrop((\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m),padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m),\n\u001b[0;32m     60\u001b[0m         T\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m---> 61\u001b[0m         \u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrivialAugmentWide\u001b[49m(),\n\u001b[0;32m     62\u001b[0m         \n\u001b[0;32m     63\u001b[0m         T\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     64\u001b[0m         T\u001b[38;5;241m.\u001b[39mRandomErasing(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m     65\u001b[0m         T\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.4914\u001b[39m, \u001b[38;5;241m0.4822\u001b[39m, \u001b[38;5;241m0.4465\u001b[39m), (\u001b[38;5;241m0.2023\u001b[39m, \u001b[38;5;241m0.1994\u001b[39m, \u001b[38;5;241m0.2010\u001b[39m))\n\u001b[0;32m     66\u001b[0m     ])\n\u001b[0;32m     67\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m CIFAR10(args\u001b[38;5;241m.\u001b[39mroot, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtrain_transform)\n\u001b[0;32m     68\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_data, args\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_workers, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchvision.transforms' has no attribute 'TrivialAugmentWide'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchmetrics.aggregation import MeanMetric\n",
    "from torchmetrics.functional.classification import accuracy\n",
    "\n",
    "from src.models import ConvNet\n",
    "from src.engines import train, evaluate\n",
    "from src.utils import load_checkpoint, save_checkpoint\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Jupyter 외 환경\n",
    "# import argparse\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--title\", type=str, default=\"augmentation\")\n",
    "# parser.add_argument(\"--device\", type=str, default=\"cuda\")\n",
    "# parser.add_argument(\"--root\", type=str, default=\"data\")\n",
    "# parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "# parser.add_argument(\"--num_workers\", type=int, default=2)\n",
    "# parser.add_argument(\"--epochs\", type=int, default=100)\n",
    "# parser.add_argument(\"--lr\", type=float, default=0.001)\n",
    "# parser.add_argument(\"--weight_decay\", type=float, default=0.0001)\n",
    "# parser.add_argument(\"--label_smoothing\", type=float, default=0.05)\n",
    "# parser.add_argument(\"--drop_rate\", type=float, default=0.1)\n",
    "# parser.add_argument(\"--logs\", type=str, default='logs')\n",
    "# parser.add_argument(\"--checkpoints\", type=str, default='checkpoints')\n",
    "# parser.add_argument(\"--resume\", type=bool, default=False)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# Jupyter 환경\n",
    "args = easydict.EasyDict({\n",
    "        \"title\" : \"augmentation\",\n",
    "        \"device\" : \"cuda\",\n",
    "        \"root\" : \"data\",\n",
    "        \"batch_size\" : 128,\n",
    "        \"num_workers\" : 2,\n",
    "        \"epochs\" : 100,\n",
    "        \"lr\" : 0.001,\n",
    "        \"weight_decay\": 0.0001,\n",
    "        \"label_smoothing\": 0.05,\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"logs\": \"logs\",\n",
    "        \"checkpoints\": \"checkpoints\",\n",
    "        \"resume\": False\n",
    "    })\n",
    "\n",
    "def main(args):\n",
    "    # Build dataset\n",
    "    train_transform = T.Compose([\n",
    "        T.RandomCrop((32, 32),padding=4),\n",
    "        T.RandomHorizontalFlip(p = 0.5),\n",
    "        T.TrivialAugmentWide(),\n",
    "        \n",
    "        T.ToTensor(),\n",
    "        T.RandomErasing(p=0.5),\n",
    "        T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    train_data = CIFAR10(args.root, train=True, download=True, transform=train_transform)\n",
    "    train_loader = DataLoader(train_data, args.batch_size, shuffle=True, num_workers=args.num_workers, drop_last=True)\n",
    "\n",
    "    val_transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    val_data = CIFAR10(args.root, train=False, download=True, transform=val_transform)\n",
    "    val_loader = DataLoader(val_data, batch_size=args.batch_size, num_workers=args.num_workers)\n",
    "\n",
    "    # Build model\n",
    "    model = ConvNet(drop_rate=args.drop_rate)\n",
    "    model = model.to(args.device)\n",
    "\n",
    "    # Build optimizer \n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    # Build scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs * len(train_loader))\n",
    "\n",
    "    # Build loss function\n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=args.label_smoothing)\n",
    "\n",
    "    # Build metric function\n",
    "    metric_fn = accuracy\n",
    "\n",
    "    # Build logger\n",
    "    train_logger = SummaryWriter(f'{args.logs}/train/{args.title}')\n",
    "    val_logger = SummaryWriter(f'{args.logs}/val/{args.title}')\n",
    "\n",
    "    # Load model\n",
    "    start_epoch = 0\n",
    "    if args.resume:\n",
    "        start_epoch = load_checkpoint(args.checkpoints, args.title, model, optimizer)\n",
    "    \n",
    "    # Main loop\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        # train one epoch\n",
    "        train_summary = train(train_loader, model, optimizer, scheduler, loss_fn, metric_fn, args.device)\n",
    "        \n",
    "        # evaluate one epoch\n",
    "        val_summary = evaluate(val_loader, model, loss_fn, metric_fn, args.device)\n",
    "\n",
    "        # write log\n",
    "        train_logger.add_scalar('Loss', train_summary['loss'], epoch + 1)\n",
    "        train_logger.add_scalar('Accuracy', train_summary['metric'], epoch + 1)\n",
    "        val_logger.add_scalar('Loss', val_summary['loss'], epoch + 1)\n",
    "        val_logger.add_scalar('Accuracy', val_summary['metric'], epoch + 1)\n",
    "\n",
    "        # save model\n",
    "        save_checkpoint(args.checkpoints, args.title, model, optimizer, epoch + 1)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "BangEnv",
   "language": "python",
   "name": "bangenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

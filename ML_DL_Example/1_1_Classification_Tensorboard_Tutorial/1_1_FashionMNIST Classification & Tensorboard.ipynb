{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613e4b5f",
   "metadata": {},
   "source": [
    "# 1. Pytorch Quickstart\n",
    "\n",
    "    1) 개요 : FashionMNIST\n",
    "        (1) 데이터셋 특징\n",
    "            1] 데이터 개수\n",
    "                - Train : 60,000\n",
    "                - Test : 10,000\n",
    "            2] 컬러 : 흑백\n",
    "            3] 라벨(클래스) 개수 : 10(0 ~ 9)\n",
    "            4] 이미지 크기 : 28 x 28\n",
    "            5] 픽셀값 : 0 ~ 255\n",
    "        (2) 데이터 가공\n",
    "            1] 원 핫 벡터 변경 : 기존 숫자 라벨 -> 원 핫 벡터\n",
    "                [1] 이유\n",
    "                    - classification은 숫자 라벨의 대소적 비교 필요없음\n",
    "                    - classficiation은 원 핫 벡터의 확률적 계산 필요\n",
    "                ex. t = 4 -> t = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]T\n",
    "        (3) 데이터 수집 이후 과정\n",
    "            1] 데이터 처리 파이프라인 생성\n",
    "                - 미니 배치 그룹화를 활용한 학습 준비\n",
    "                [1] inputs, labels 불러오기\n",
    "                [2] inputs, labels 전처리\n",
    "                [3] inputs, labels를 미니배치로 그룹화\n",
    "            2] 모델 생성\n",
    "                - input을 활용하여 예측\n",
    "                [1] 모델 선정\n",
    "                    ex. 뉴럴 네트워크, SVM 등\n",
    "                [2] 모델의 하이퍼파라미터 세팅\n",
    "                    ex. 뉴런의 개수, 레이어 개수 등\n",
    "            3] 학습, 평가 loop\n",
    "                - 학습 loop : labels와 predictions 사이에 손실 계산 + trainning 데이터를 활용한 모델의 파라미터 업데이트\n",
    "                - 평가 loop : validation 데이터를 활용한 모델의 성능 측정\n",
    "            4] 모델 저장 or 불러오기\n",
    "                - 학습된 모델 저장 or 불러오기\n",
    "    2) Pytorch QuickStart\n",
    "        (1) 개념\n",
    "            - 머신러닝 오픈소스 프레임워크\n",
    "        (2) 특징\n",
    "            1] Pytorch\n",
    "                - 배포 : Meta\n",
    "                - 사용자층 : 많음\n",
    "                - 커스터마이징 : 더 자유로운 커스터마이징 but 조금 더 어려움\n",
    "            2] Tensorflow\n",
    "                - 배포 : Google\n",
    "                - 사용자층 : 보통\n",
    "                - 커스터마이징 : 다소 어려운 커스터마이징 but 조금 더 쉬움\n",
    "                - 대규모 배포가 필요한 환경에서 주로 활용\n",
    "                    ex. 웹, 서버, 임베디드 등\n",
    "        (3) 역사 of 라이브러리\n",
    "            [1] caffe\n",
    "                - CUDA 코딩 매우 번거로움\n",
    "            [2] Tensorflow 1.x\n",
    "                - 알아서 GPU를 활용한 연산\n",
    "            [3] Tensorflow 2.x & Pytorch\n",
    "                - 편의성 대폭 향상\n",
    "                - 배포 순서 : Pytorch -> Tensorflow 2.x(Pytorch가 먼저 배포되어 사용자층 많음)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5928547",
   "metadata": {},
   "source": [
    "        (1) import Pytorch packages\n",
    "            1] torch.nn : 모델 설계 도움\n",
    "                ex. fully connected layer, convolution layer, sigmoid activation\n",
    "            2] torch.optim : 최적화 알고리즘 + Sheduler\n",
    "                ex. stochastic garadient descent, adam + learning rate scheduling\n",
    "            3] Dataset : data processing pipeline 설계\n",
    "            4] DataLoader : data processing pipeline 설계\n",
    "\n",
    "        (2) Configuration(config)\n",
    "            - 데이터셋 경로, 배치 사이즈, learning rate, maximum traing epoch 등\n",
    "\n",
    "        (3) Build data processing pipeline\n",
    "            1] torch.utils.data.Dataset\n",
    "                - 데이터셋 : 샘플, 대응 라벨 저장\n",
    "                - 데이터 로드 -> 전처리 과정 지원\n",
    "            2] torch.utils.data.DataLoader\n",
    "                - 데이터로더 : 데이터셋 주위를 감싼다\n",
    "                - mini batch 데이터 생성\n",
    "\n",
    "        (4) Build model\n",
    "            1] torch.nn.Module\n",
    "                [1] __init__ : 네트워크 레이어 정의\n",
    "                [2] forward : 네트워크를 통해 데이터가 어떻게 통과할 것인지 구체화\n",
    "\n",
    "        (5) Build optimizer + scheduler + loss function + metric function\n",
    "\n",
    "        (6) Build a training loop\n",
    "            1] Step\n",
    "                [1] DataLoader로부터 batch data 로드\n",
    "                [2] Forward propagation\n",
    "                [3] Backward propagation\n",
    "                [4] Update statistics\n",
    "\n",
    "        (7) Build an evaluation loop\n",
    "            1] Step\n",
    "                [1] DataLoader로부터 batch data 로드\n",
    "                [2] Forward propagation\n",
    "                [3] Update statistics\n",
    "                - val data로 성능을 검증해야하므로, 학습을 위한 Backward propagation은 하지 않는다\n",
    "\n",
    "        (8) Run traning / evaluation loop\n",
    "            1] Step\n",
    "                [1] DataLoader로부터 batch data 로드\n",
    "                [2] Forward propagation\n",
    "                [3] Update statistics\n",
    "                - val data로 성능을 검증해야하므로, 학습을 위한 Backward propagation은 하지 않는다\n",
    "                \n",
    "        - state_dict : model의 파라미터, optimizers를 key, value 형태로 기록한 dict\n",
    "        - Save checkpoint\n",
    "        - Load checkpoint\n",
    "        - Resume training from checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3e1d9a",
   "metadata": {},
   "source": [
    "# 2. Tensorboard\n",
    "    1) 특징 : Tensorflow의 visualization toolkit(Pytorch도 지원)\n",
    "        - images, text, audio data 시각화\n",
    "        - low dimensional space로 embedding\n",
    "        - model graph 시각화\n",
    "        - weights와 bias histogram 시각화\n",
    "        - metrics(loss, accuracy) 변화 시각화\n",
    "        \n",
    "    2) Tensorboard Quickstart\n",
    "        (1) import tensorboard package\n",
    "        (2) Configuration(config)\n",
    "    (3) tensorboard로 model training tracking\n",
    "        - SummaryWriter(f'{log_dir}/train').add_scalar(~~)\n",
    "        - SummaryWriter(f'{log_dir}/val').add_scalar(~~)\n",
    "        - conda prompt -> tensorboard --logdir=log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "084df06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 현재 가상환경 패키지 확인\n",
    "# !pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85c19723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CPython 패키지 설치\n",
    "# !pip install CPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1c23bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # numpy 패키지 설치\n",
    "# !pip install numpy\n",
    "# !pip install numpy==1.21.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5dfd655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # numpy 패키지 업그레이드\n",
    "# !pip install --upgrade --force-reinstall numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d13a2844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # torchmetrics 패키지 설치\n",
    "# !pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6ba280d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "# torch 설치 + GPU 확인\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d72ed3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) import Pytorch packages\n",
    "#     1] torch.nn : 모델 설계 도움\n",
    "#         ex. fully connected layer, convolution layer, sigmoid activation\n",
    "#     2] torch.optim : 최적화 알고리즘 + Sheduler\n",
    "#         ex. stochastic garadient descent, adam + learning rate scheduling\n",
    "#     3] Dataset : data processing pipeline 설계\n",
    "#     4] DataLoader : data processing pipeline 설계\n",
    "#     5] SummaryWriter : tensorboard 기록 저장\n",
    "\n",
    "# (1) import Tensorboard package\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# torchvision : 컴퓨터 비전\n",
    "# torchaudio : 컴퓨터 음성\n",
    "# torchtext : 컴퓨터 텍스트\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader # data processing pipeline 설계\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchmetrics.aggregation import MeanMetric # accuracy의 평균을 구하기 위해\n",
    "from torchmetrics.functional.classification import accuracy # 데이터 1개의 예측과 실제 라벨간 accuracy 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bbc3929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Configuration(config)\n",
    "#     - 데이터셋 경로, 배치 사이즈, learning rate, maximum traing epoch 등\n",
    "\n",
    "# (2) Configuration(config)\n",
    "\n",
    "# Build Config\n",
    "title = 'fashionmnist_quickstart'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "data_root = 'data'\n",
    "batch_size = 64\n",
    "base_lr = 0.001\n",
    "epochs = 20\n",
    "log_dir = 'log'\n",
    "checkpoint_dir = 'checkpoint'\n",
    "\n",
    "# Build directory\n",
    "os.makedirs(log_dir, exist_ok = True) # 없어도 무방한 코드\n",
    "os.makedirs(checkpoint_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62348507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Build data processing pipeline\n",
    "#     1] torch.utils.data.Dataset\n",
    "#         - 데이터셋 : 샘플, 대응 라벨 저장\n",
    "#         - 데이터 로드 -> 전처리 과정 지원\n",
    "#     2] torch.utils.data.DataLoader\n",
    "#         - 데이터로더 : 데이터셋 주위를 감싼다\n",
    "#         - mini batch 데이터 생성\n",
    "\n",
    "# Build Dataset\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(), # PIL Image -> Tensor\n",
    "    T.Normalize((0.5,), (0.5,)) # mean(0 ~ 1) -> std(-0.5 ~ 0.5)\n",
    "])\n",
    "train_data = FashionMNIST(data_root, train=True, download=True, transform=transform)\n",
    "# data_root : 데이터셋 저장 위치\n",
    "# train : train data인지 test data인지 설정\n",
    "# download : 데이터셋 없는 경우 다운로드 할지 말지 결정\n",
    "# transform : 데이터 전처리 설정\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_data = FashionMNIST(data_root, train=False, download=True, transform=transform)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cd5c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) Build model\n",
    "#     1] torch.nn.Module\n",
    "#         [1] __init__ : 네트워크 레이어 정의\n",
    "#         [2] forward : 네트워크를 통해 데이터가 어떻게 통과할 것인지 구체화\n",
    "\n",
    "# Define model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self): # 784 -> 512 -> 512 -> 10\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "        self.act = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = x.reshape((x.shape[0], -1)) # 1개 열에 전부 표시 # (64, 1, 28, 28) -> (64, 784)\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "# Build model\n",
    "model = MLP()\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86f995b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) Build optimizer + scheduler + loss function + metric function\n",
    "\n",
    "# Build optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=base_lr)\n",
    "\n",
    "# Build scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs * len(train_loader))\n",
    "# Cosine : 머신러닝은 하이퍼파라미터 튜닝이 매우 많음. scheduler에서도 하이퍼파라미터 튜닝이 필요한데, cosine함수는 base learning rate만 하이퍼파라미터로 존재하므로 사용하기 편리 + 성능 좋음\n",
    "\n",
    "# Build loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Build metric function\n",
    "metric_fn = accuracy\n",
    "\n",
    "# Build logger\n",
    "train_logger = SummaryWriter(f'{log_dir}/train')\n",
    "val_logger = SummaryWriter(f'{log_dir}/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5872df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (6) Build a training loop\n",
    "#     1] Step\n",
    "#         [1] DataLoader로부터 batch data 로드\n",
    "#         [2] Forward propagation\n",
    "#         [3] Backward propagation\n",
    "#         [4] Update statistics\n",
    "\n",
    "# Define training function\n",
    "def train(loader, model, optimizer, scheduler, loss_fn, metric_fn, device):\n",
    "    # Set model to train mode\n",
    "    model.train() # nn.Module의 layer에 따라 train mode와 test mode가 다르게 동작하는 경우 존재하기 때문에 train mode인지 test mode인지 설정 필요\n",
    "    \n",
    "    # Create average meters to measure loss and metrics + Move metric to device\n",
    "    loss_mean = MeanMetric().to(device)\n",
    "    metric_mean = MeanMetric().to(device)\n",
    "    \n",
    "    # train model for one epoch\n",
    "    for inputs, targets in loader:\n",
    "        # Move data to device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        metric = metric_fn(outputs, targets)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # 모델 파라미터 업데이트\n",
    "        optimizer.step() # optimizer 로그 남김 + 다음 상태로 이동\n",
    "        \n",
    "        # Update statistics\n",
    "        loss_mean.update(loss)\n",
    "        metric_mean.update(metric)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "    # Summarize statistics(Dic)\n",
    "    summary = {'loss': loss_mean.compute(), 'metric': metric_mean.compute()}\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4b89c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (7) Build an evaluation loop\n",
    "#     1] Step\n",
    "#         [1] DataLoader로부터 batch data 로드\n",
    "#         [2] Forward propagation\n",
    "#         [3] Update statistics\n",
    "#         - val data로 성능을 검증해야하므로, 학습을 위한 Backward propagation은 하지 않는다\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate(loader, model, loss_fn, metric_fn, device):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Create average meters to measure loss and accuracy + Move metric to device\n",
    "    loss_mean = MeanMetric().to(device)\n",
    "    metric_mean = MeanMetric().to(device)\n",
    "    \n",
    "    # Evaluate model for one epoch\n",
    "    for inputs, targets in loader:\n",
    "        # Move data to device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        with torch.no_grad(): # evaluation : grad 계산x. 계산해도 무방하지만 GPU 자원을 아끼기 위해 계산x\n",
    "            outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        metric = metric_fn(outputs, targets) \n",
    "        \n",
    "        # Update statistics\n",
    "        loss_mean.update(loss)\n",
    "        metric_mean.update(metric)\n",
    "        \n",
    "    # Summarize statistics(Dic)\n",
    "    summary = {'loss' : loss_mean.compute(), 'metric' : metric_mean.compute()}\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93f627af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 0.4813, Train Accuracy 0.8228, Test Loss 0.4254, Test Accuracy 0.8496\n",
      "Epoch 2: Train Loss 0.3635, Train Accuracy 0.8652, Test Loss 0.3889, Test Accuracy 0.8577\n",
      "Epoch 3: Train Loss 0.3224, Train Accuracy 0.8804, Test Loss 0.3716, Test Accuracy 0.8644\n",
      "Epoch 4: Train Loss 0.2975, Train Accuracy 0.8887, Test Loss 0.3567, Test Accuracy 0.8708\n",
      "Epoch 5: Train Loss 0.2723, Train Accuracy 0.8977, Test Loss 0.3358, Test Accuracy 0.8818\n",
      "Epoch 6: Train Loss 0.2509, Train Accuracy 0.9043, Test Loss 0.3457, Test Accuracy 0.8767\n",
      "Epoch 7: Train Loss 0.2318, Train Accuracy 0.9120, Test Loss 0.3302, Test Accuracy 0.8825\n",
      "Epoch 8: Train Loss 0.2117, Train Accuracy 0.9192, Test Loss 0.3333, Test Accuracy 0.8899\n",
      "Epoch 9: Train Loss 0.1931, Train Accuracy 0.9267, Test Loss 0.3422, Test Accuracy 0.8876\n",
      "Epoch 10: Train Loss 0.1728, Train Accuracy 0.9334, Test Loss 0.3351, Test Accuracy 0.8939\n",
      "Epoch 11: Train Loss 0.1544, Train Accuracy 0.9411, Test Loss 0.3492, Test Accuracy 0.8959\n",
      "Epoch 12: Train Loss 0.1354, Train Accuracy 0.9486, Test Loss 0.3424, Test Accuracy 0.8952\n",
      "Epoch 13: Train Loss 0.1187, Train Accuracy 0.9550, Test Loss 0.3515, Test Accuracy 0.8978\n",
      "Epoch 14: Train Loss 0.1022, Train Accuracy 0.9620, Test Loss 0.3615, Test Accuracy 0.8982\n",
      "Epoch 15: Train Loss 0.0894, Train Accuracy 0.9675, Test Loss 0.3684, Test Accuracy 0.8971\n",
      "Epoch 16: Train Loss 0.0780, Train Accuracy 0.9729, Test Loss 0.3779, Test Accuracy 0.8982\n",
      "Epoch 17: Train Loss 0.0693, Train Accuracy 0.9767, Test Loss 0.3849, Test Accuracy 0.8979\n",
      "Epoch 18: Train Loss 0.0633, Train Accuracy 0.9794, Test Loss 0.3905, Test Accuracy 0.8983\n",
      "Epoch 19: Train Loss 0.0597, Train Accuracy 0.9810, Test Loss 0.3868, Test Accuracy 0.8999\n",
      "Epoch 20: Train Loss 0.0579, Train Accuracy 0.9819, Test Loss 0.3885, Test Accuracy 0.8996\n"
     ]
    }
   ],
   "source": [
    "# (8) Run traning / evaluation loop\n",
    "#     1] Step\n",
    "#         [1] DataLoader로부터 batch data 로드\n",
    "#         [2] Forward propagation\n",
    "#         [3] Update statistics\n",
    "#         - val data로 성능을 검증해야하므로, 학습을 위한 Backward propagation은 하지 않는다\n",
    "\n",
    "# (3) tensorboard로 model training tracking\n",
    "#     - SummaryWriter(f'{log_dir}/train').add_scalar(~~)\n",
    "#     - SummaryWriter(f'{log_dir}/val').add_scalar(~~)\n",
    "#     - conda prompt -> tensorboard --logdir=log\n",
    "\n",
    "# Main loop\n",
    "for epoch in range(epochs):\n",
    "    # Train one epoch\n",
    "    train_summary = train(train_loader, model, optimizer, scheduler, loss_fn, metric_fn, device)\n",
    "    \n",
    "    # Evaluate one epoch\n",
    "    val_summary = evaluate(val_loader, model, loss_fn, metric_fn, device)\n",
    "    \n",
    "    # Print log\n",
    "    print((\n",
    "        f'Epoch {epoch + 1}: '\n",
    "        + f'Train Loss {train_summary[\"loss\"]:.04f}, '\n",
    "        + f'Train Accuracy {train_summary[\"metric\"]:.04f}, '\n",
    "        + f'Test Loss {val_summary[\"loss\"]:.04f}, '\n",
    "        + f'Test Accuracy {val_summary[\"metric\"]:.04f}'\n",
    "    ))\n",
    "    \n",
    "    # Write log\n",
    "    train_logger.add_scalar('Loss', train_summary['loss'], epoch + 1)\n",
    "    train_logger.add_scalar('Accuracy', train_summary['metric'], epoch + 1)\n",
    "    val_logger.add_scalar('Loss', val_summary['loss'], epoch + 1)\n",
    "    val_logger.add_scalar('Accuracy', val_summary['metric'], epoch + 1)\n",
    "    \n",
    "    # Save model\n",
    "    state_dict = {\n",
    "        'epoch': epoch + 1,\n",
    "        'model': model.state_dict(), # 얘는 뭐지???\n",
    "        'optimizer': optimizer.state_dict(), \n",
    "    }\n",
    "    checkpoint_path = f'{checkpoint_dir}/{title}_last.pth'\n",
    "    torch.save(state_dict, checkpoint_path)\n",
    "    \n",
    "train_logger.close()\n",
    "val_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c27c7749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dict : model의 파라미터, optimizers를 key, value 형태로 기록한 dict\n",
    "# Save checkpoint\n",
    "# Load checkpoint\n",
    "# Resume training from checkpoint\n",
    "\n",
    "# Load model\n",
    "model_pretrained = MLP()\n",
    "\n",
    "checkpoint_path = f'{checkpoint_dir}/{title}_last.pth'\n",
    "state_dict = torch.load(checkpoint_path) # 경로 상의 state_dict 불러오기\n",
    "\n",
    "model_pretrained.load_state_dict(state_dict['model']) # state_dict 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "327d5cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[random] Test Acc 0.0771\n",
      "[Pretrained] Test Acc 0.8996\n"
     ]
    }
   ],
   "source": [
    "# Comparison with randomly initialized model\n",
    "model_random = MLP()\n",
    "\n",
    "model_random.to(device)\n",
    "model_pretrained.to(device)\n",
    "\n",
    "random_summary = evaluate(val_loader, model_random, loss_fn, metric_fn, device)\n",
    "pretrained_summary = evaluate(val_loader, model_pretrained, loss_fn, metric_fn, device)\n",
    "\n",
    "print(f'[random] Test Acc {random_summary[\"metric\"]:.04f}')\n",
    "print(f'[Pretrained] Test Acc {pretrained_summary[\"metric\"]:.04f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25a2502a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MLP:\n\tMissing key(s) in state_dict: \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\". \n\tUnexpected key(s) in state_dict: \"state\", \"param_groups\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m start_epoch \u001b[38;5;241m=\u001b[39m state_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda\\envs\\BangEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1223\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1218\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   1219\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1220\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1224\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MLP:\n\tMissing key(s) in state_dict: \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\". \n\tUnexpected key(s) in state_dict: \"state\", \"param_groups\". "
     ]
    }
   ],
   "source": [
    "# Load model and optimizer states\n",
    "checkpoint_path = f'{checkpoint_dir}/{title}_last.pth'\n",
    "state_dict = torch.load(checkpoint_path)\n",
    "                        \n",
    "start_epoch = state_dict['epoch']\n",
    "model.load_state_dict(state_dict['model'])\n",
    "optimizer.load_state_dict(state_dict['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0df1198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    # train one epoch\n",
    "    train_summary = train(train_loader, model, optimizer, scheduler, loss_fn, metric_fn, device)\n",
    "    \n",
    "    # evaluate one epoch\n",
    "    val_summary = evaluate(val_loader, model, loss_fn, metric_fn, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BangEnv",
   "language": "python",
   "name": "bangenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
